{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and GLoVe modeling with PCA and cleaned dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talos as ta\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from talos.model.early_stopper import early_stopper\n",
    "from talos.model.normalizers import lr_normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('updated_merged_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>retweet_from</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>encoded_sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "      <th>pc_1</th>\n",
       "      <th>pc_2</th>\n",
       "      <th>pc_3</th>\n",
       "      <th>pc_4</th>\n",
       "      <th>...</th>\n",
       "      <th>pc_30</th>\n",
       "      <th>pc_31</th>\n",
       "      <th>pc_32</th>\n",
       "      <th>pc_33</th>\n",
       "      <th>pc_34</th>\n",
       "      <th>pc_35</th>\n",
       "      <th>pc_36</th>\n",
       "      <th>pc_37</th>\n",
       "      <th>pc_38</th>\n",
       "      <th>signal_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>assuming acceleration of to but in a comfortab...</td>\n",
       "      <td>-0.031750</td>\n",
       "      <td>-0.028850</td>\n",
       "      <td>-0.030290</td>\n",
       "      <td>-0.018736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017885</td>\n",
       "      <td>0.045822</td>\n",
       "      <td>-0.006862</td>\n",
       "      <td>-0.012954</td>\n",
       "      <td>-0.005468</td>\n",
       "      <td>-0.100572</td>\n",
       "      <td>-0.029374</td>\n",
       "      <td>0.041363</td>\n",
       "      <td>0.035932</td>\n",
       "      <td>stay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>is capable of transporting satellite to orbit ...</td>\n",
       "      <td>-0.059331</td>\n",
       "      <td>-0.099826</td>\n",
       "      <td>-0.154772</td>\n",
       "      <td>-0.040355</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011667</td>\n",
       "      <td>0.003882</td>\n",
       "      <td>0.018002</td>\n",
       "      <td>-0.019813</td>\n",
       "      <td>-0.035809</td>\n",
       "      <td>-0.034079</td>\n",
       "      <td>-0.014687</td>\n",
       "      <td>-0.008986</td>\n",
       "      <td>-0.019845</td>\n",
       "      <td>stay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>171</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>yup</td>\n",
       "      <td>-0.002330</td>\n",
       "      <td>0.033983</td>\n",
       "      <td>0.069531</td>\n",
       "      <td>-0.014177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015722</td>\n",
       "      <td>-0.025726</td>\n",
       "      <td>0.065735</td>\n",
       "      <td>0.041686</td>\n",
       "      <td>-0.021751</td>\n",
       "      <td>-0.014138</td>\n",
       "      <td>-0.037047</td>\n",
       "      <td>-0.050031</td>\n",
       "      <td>-0.084722</td>\n",
       "      <td>stay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>171</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>part</td>\n",
       "      <td>-0.005050</td>\n",
       "      <td>0.024117</td>\n",
       "      <td>0.043362</td>\n",
       "      <td>-0.001804</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009052</td>\n",
       "      <td>-0.008601</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>-0.007619</td>\n",
       "      <td>-0.005289</td>\n",
       "      <td>-0.025700</td>\n",
       "      <td>-0.008989</td>\n",
       "      <td>-0.011967</td>\n",
       "      <td>-0.004665</td>\n",
       "      <td>stay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>171</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>fly to most place on earth in under min and an...</td>\n",
       "      <td>-0.031167</td>\n",
       "      <td>-0.027725</td>\n",
       "      <td>-0.012331</td>\n",
       "      <td>-0.039909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020042</td>\n",
       "      <td>0.030526</td>\n",
       "      <td>-0.039462</td>\n",
       "      <td>0.088186</td>\n",
       "      <td>-0.012277</td>\n",
       "      <td>0.033849</td>\n",
       "      <td>0.040576</td>\n",
       "      <td>0.051975</td>\n",
       "      <td>-0.031236</td>\n",
       "      <td>stay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  retweet_from  tweet_length  encoded_sentiment  polarity  \\\n",
       "0           0           171           105                  1  0.366667   \n",
       "1           1            77           113                  1  0.200000   \n",
       "2           2           171             6                  0  0.000000   \n",
       "3           3           171             7                  0  0.000000   \n",
       "4           4           171            96                  1  0.650000   \n",
       "\n",
       "                                               tweet      pc_1      pc_2  \\\n",
       "0  assuming acceleration of to but in a comfortab... -0.031750 -0.028850   \n",
       "1  is capable of transporting satellite to orbit ... -0.059331 -0.099826   \n",
       "2                                                yup -0.002330  0.033983   \n",
       "3                                               part -0.005050  0.024117   \n",
       "4  fly to most place on earth in under min and an... -0.031167 -0.027725   \n",
       "\n",
       "       pc_3      pc_4    ...        pc_30     pc_31     pc_32     pc_33  \\\n",
       "0 -0.030290 -0.018736    ...     0.017885  0.045822 -0.006862 -0.012954   \n",
       "1 -0.154772 -0.040355    ...    -0.011667  0.003882  0.018002 -0.019813   \n",
       "2  0.069531 -0.014177    ...     0.015722 -0.025726  0.065735  0.041686   \n",
       "3  0.043362 -0.001804    ...    -0.009052 -0.008601  0.001285 -0.007619   \n",
       "4 -0.012331 -0.039909    ...     0.020042  0.030526 -0.039462  0.088186   \n",
       "\n",
       "      pc_34     pc_35     pc_36     pc_37     pc_38  signal_y  \n",
       "0 -0.005468 -0.100572 -0.029374  0.041363  0.035932      stay  \n",
       "1 -0.035809 -0.034079 -0.014687 -0.008986 -0.019845      stay  \n",
       "2 -0.021751 -0.014138 -0.037047 -0.050031 -0.084722      stay  \n",
       "3 -0.005289 -0.025700 -0.008989 -0.011967 -0.004665      stay  \n",
       "4 -0.012277  0.033849  0.040576  0.051975 -0.031236      stay  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['Unnamed: 0'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the tweet column to a string to account of data type errors\n",
    "df.tweet = df.tweet.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the class balance of our target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.signal_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    stay\n",
       "1    stay\n",
       "2    stay\n",
       "3    stay\n",
       "4    stay\n",
       "Name: signal_y, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stay    1311\n",
       "up       993\n",
       "down     900\n",
       "Name: signal_y, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"signal_y\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stay    0.409176\n",
       "up      0.309925\n",
       "down    0.280899\n",
       "Name: signal_y, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"signal_y\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the target to an array of dummies for our three classes (stay, up, down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We use this y (the target) for all of our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(target).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(y))\n",
    "y[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Model for non tweet features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the dataframe for not tweet features, which include the tf-idf of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_tweet = df.drop(['signal_y', 'tweet'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweet_from</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>encoded_sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>pc_1</th>\n",
       "      <th>pc_2</th>\n",
       "      <th>pc_3</th>\n",
       "      <th>pc_4</th>\n",
       "      <th>pc_5</th>\n",
       "      <th>pc_6</th>\n",
       "      <th>...</th>\n",
       "      <th>pc_29</th>\n",
       "      <th>pc_30</th>\n",
       "      <th>pc_31</th>\n",
       "      <th>pc_32</th>\n",
       "      <th>pc_33</th>\n",
       "      <th>pc_34</th>\n",
       "      <th>pc_35</th>\n",
       "      <th>pc_36</th>\n",
       "      <th>pc_37</th>\n",
       "      <th>pc_38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>-0.031750</td>\n",
       "      <td>-0.028850</td>\n",
       "      <td>-0.030290</td>\n",
       "      <td>-0.018736</td>\n",
       "      <td>-0.060675</td>\n",
       "      <td>0.057798</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002729</td>\n",
       "      <td>0.017885</td>\n",
       "      <td>0.045822</td>\n",
       "      <td>-0.006862</td>\n",
       "      <td>-0.012954</td>\n",
       "      <td>-0.005468</td>\n",
       "      <td>-0.100572</td>\n",
       "      <td>-0.029374</td>\n",
       "      <td>0.041363</td>\n",
       "      <td>0.035932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.059331</td>\n",
       "      <td>-0.099826</td>\n",
       "      <td>-0.154772</td>\n",
       "      <td>-0.040355</td>\n",
       "      <td>0.072285</td>\n",
       "      <td>0.209163</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059361</td>\n",
       "      <td>-0.011667</td>\n",
       "      <td>0.003882</td>\n",
       "      <td>0.018002</td>\n",
       "      <td>-0.019813</td>\n",
       "      <td>-0.035809</td>\n",
       "      <td>-0.034079</td>\n",
       "      <td>-0.014687</td>\n",
       "      <td>-0.008986</td>\n",
       "      <td>-0.019845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>171</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002330</td>\n",
       "      <td>0.033983</td>\n",
       "      <td>0.069531</td>\n",
       "      <td>-0.014177</td>\n",
       "      <td>-0.037774</td>\n",
       "      <td>-0.071279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087971</td>\n",
       "      <td>0.015722</td>\n",
       "      <td>-0.025726</td>\n",
       "      <td>0.065735</td>\n",
       "      <td>0.041686</td>\n",
       "      <td>-0.021751</td>\n",
       "      <td>-0.014138</td>\n",
       "      <td>-0.037047</td>\n",
       "      <td>-0.050031</td>\n",
       "      <td>-0.084722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>171</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.005050</td>\n",
       "      <td>0.024117</td>\n",
       "      <td>0.043362</td>\n",
       "      <td>-0.001804</td>\n",
       "      <td>-0.016279</td>\n",
       "      <td>-0.052016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015861</td>\n",
       "      <td>-0.009052</td>\n",
       "      <td>-0.008601</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>-0.007619</td>\n",
       "      <td>-0.005289</td>\n",
       "      <td>-0.025700</td>\n",
       "      <td>-0.008989</td>\n",
       "      <td>-0.011967</td>\n",
       "      <td>-0.004665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>171</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>-0.031167</td>\n",
       "      <td>-0.027725</td>\n",
       "      <td>-0.012331</td>\n",
       "      <td>-0.039909</td>\n",
       "      <td>-0.070462</td>\n",
       "      <td>0.019591</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081234</td>\n",
       "      <td>0.020042</td>\n",
       "      <td>0.030526</td>\n",
       "      <td>-0.039462</td>\n",
       "      <td>0.088186</td>\n",
       "      <td>-0.012277</td>\n",
       "      <td>0.033849</td>\n",
       "      <td>0.040576</td>\n",
       "      <td>0.051975</td>\n",
       "      <td>-0.031236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   retweet_from  tweet_length  encoded_sentiment  polarity      pc_1  \\\n",
       "0           171           105                  1  0.366667 -0.031750   \n",
       "1            77           113                  1  0.200000 -0.059331   \n",
       "2           171             6                  0  0.000000 -0.002330   \n",
       "3           171             7                  0  0.000000 -0.005050   \n",
       "4           171            96                  1  0.650000 -0.031167   \n",
       "\n",
       "       pc_2      pc_3      pc_4      pc_5      pc_6    ...        pc_29  \\\n",
       "0 -0.028850 -0.030290 -0.018736 -0.060675  0.057798    ...    -0.002729   \n",
       "1 -0.099826 -0.154772 -0.040355  0.072285  0.209163    ...    -0.059361   \n",
       "2  0.033983  0.069531 -0.014177 -0.037774 -0.071279    ...     0.087971   \n",
       "3  0.024117  0.043362 -0.001804 -0.016279 -0.052016    ...     0.015861   \n",
       "4 -0.027725 -0.012331 -0.039909 -0.070462  0.019591    ...    -0.081234   \n",
       "\n",
       "      pc_30     pc_31     pc_32     pc_33     pc_34     pc_35     pc_36  \\\n",
       "0  0.017885  0.045822 -0.006862 -0.012954 -0.005468 -0.100572 -0.029374   \n",
       "1 -0.011667  0.003882  0.018002 -0.019813 -0.035809 -0.034079 -0.014687   \n",
       "2  0.015722 -0.025726  0.065735  0.041686 -0.021751 -0.014138 -0.037047   \n",
       "3 -0.009052 -0.008601  0.001285 -0.007619 -0.005289 -0.025700 -0.008989   \n",
       "4  0.020042  0.030526 -0.039462  0.088186 -0.012277  0.033849  0.040576   \n",
       "\n",
       "      pc_37     pc_38  \n",
       "0  0.041363  0.035932  \n",
       "1 -0.008986 -0.019845  \n",
       "2 -0.050031 -0.084722  \n",
       "3 -0.011967 -0.004665  \n",
       "4  0.051975 -0.031236  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roberthillery/anaconda3/envs/venv/lib/python2.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# these are the features for the no_tweet ANN model\n",
    "X = df_no_tweet.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.71000000e+02,  1.05000000e+02,  1.00000000e+00,\n",
       "         3.66666667e-01, -3.17503779e-02, -2.88503208e-02,\n",
       "        -3.02899383e-02, -1.87361455e-02, -6.06745282e-02,\n",
       "         5.77981532e-02, -1.24514649e-01,  1.16679040e-01,\n",
       "         5.94730409e-02, -2.91580810e-02, -4.82227257e-02,\n",
       "         7.24178428e-02,  3.90785157e-02,  2.93682030e-02,\n",
       "        -5.01187584e-02,  1.81687862e-02,  1.42445412e-02,\n",
       "        -3.82909349e-02,  9.11399155e-03, -3.36043675e-02,\n",
       "         3.00319803e-03,  1.42111843e-02,  1.02493801e-02,\n",
       "         1.01547097e-02,  3.73028256e-02, -1.27392087e-02,\n",
       "         3.30800335e-03, -4.88425818e-02, -2.72917718e-03,\n",
       "         1.78852054e-02,  4.58221637e-02, -6.86176435e-03,\n",
       "        -1.29544357e-02, -5.46828939e-03, -1.00572163e-01,\n",
       "        -2.93740249e-02,  4.13625539e-02,  3.59321462e-02],\n",
       "       [ 7.70000000e+01,  1.13000000e+02,  1.00000000e+00,\n",
       "         2.00000000e-01, -5.93306206e-02, -9.98259555e-02,\n",
       "        -1.54772144e-01, -4.03554083e-02,  7.22851718e-02,\n",
       "         2.09163426e-01, -8.00307351e-02,  7.77922425e-02,\n",
       "        -7.33890794e-02, -8.92879922e-02,  3.86667278e-02,\n",
       "         6.68178386e-02,  5.67030796e-03,  1.43317685e-02,\n",
       "        -1.94389487e-02,  5.42790281e-02, -1.06515944e-01,\n",
       "         2.14890229e-01,  3.64107528e-02,  1.52512417e-03,\n",
       "         5.25963522e-02,  1.93625545e-02, -9.19316254e-02,\n",
       "        -3.80315566e-02,  1.43679493e-02, -4.18951930e-03,\n",
       "         6.96910140e-04, -9.49459749e-02, -5.93606627e-02,\n",
       "        -1.16671812e-02,  3.88158805e-03,  1.80024713e-02,\n",
       "        -1.98131687e-02, -3.58088107e-02, -3.40787869e-02,\n",
       "        -1.46867250e-02, -8.98559724e-03, -1.98452008e-02]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# this is check the target for the Neural Network\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2307 samples, validate on 257 samples\n",
      "Epoch 1/100\n",
      "2307/2307 [==============================] - 2s 715us/step - loss: 0.6321 - acc: 0.6668 - val_loss: 0.6286 - val_acc: 0.6667\n",
      "Epoch 2/100\n",
      "2307/2307 [==============================] - 1s 433us/step - loss: 0.6307 - acc: 0.6668 - val_loss: 0.6287 - val_acc: 0.6667\n",
      "Epoch 3/100\n",
      "2307/2307 [==============================] - 1s 347us/step - loss: 0.6300 - acc: 0.6668 - val_loss: 0.6282 - val_acc: 0.6667\n",
      "Epoch 4/100\n",
      "2307/2307 [==============================] - 1s 330us/step - loss: 0.6297 - acc: 0.6668 - val_loss: 0.6290 - val_acc: 0.6667: 0s - loss: 0.6300 - acc: 0.666\n",
      "Epoch 5/100\n",
      "2307/2307 [==============================] - 1s 304us/step - loss: 0.6302 - acc: 0.6668 - val_loss: 0.6293 - val_acc: 0.6667\n",
      "Epoch 6/100\n",
      "2307/2307 [==============================] - 1s 309us/step - loss: 0.6303 - acc: 0.6668 - val_loss: 0.6282 - val_acc: 0.6667\n",
      "Epoch 7/100\n",
      "2307/2307 [==============================] - 1s 337us/step - loss: 0.6298 - acc: 0.6668 - val_loss: 0.6306 - val_acc: 0.6667\n",
      "Epoch 8/100\n",
      "2307/2307 [==============================] - 1s 365us/step - loss: 0.6298 - acc: 0.6668 - val_loss: 0.6293 - val_acc: 0.6667\n",
      "Epoch 9/100\n",
      "2307/2307 [==============================] - 1s 366us/step - loss: 0.6302 - acc: 0.6670 - val_loss: 0.6286 - val_acc: 0.6667\n",
      "Epoch 10/100\n",
      "2307/2307 [==============================] - 1s 338us/step - loss: 0.6295 - acc: 0.6668 - val_loss: 0.6287 - val_acc: 0.6667\n",
      "Epoch 11/100\n",
      "2307/2307 [==============================] - 1s 345us/step - loss: 0.6298 - acc: 0.6668 - val_loss: 0.6289 - val_acc: 0.6667\n",
      "Epoch 12/100\n",
      "2307/2307 [==============================] - 1s 370us/step - loss: 0.6294 - acc: 0.6668 - val_loss: 0.6282 - val_acc: 0.6667\n",
      "Epoch 13/100\n",
      "2307/2307 [==============================] - 1s 298us/step - loss: 0.6296 - acc: 0.6668 - val_loss: 0.6287 - val_acc: 0.6667\n",
      "Epoch 14/100\n",
      "2307/2307 [==============================] - 1s 286us/step - loss: 0.6294 - acc: 0.6668 - val_loss: 0.6291 - val_acc: 0.6667\n",
      "Epoch 15/100\n",
      "2307/2307 [==============================] - 1s 291us/step - loss: 0.6297 - acc: 0.6668 - val_loss: 0.6291 - val_acc: 0.6667\n",
      "Epoch 16/100\n",
      "2307/2307 [==============================] - 1s 268us/step - loss: 0.6290 - acc: 0.6668 - val_loss: 0.6289 - val_acc: 0.6667\n",
      "Epoch 17/100\n",
      "2307/2307 [==============================] - 1s 270us/step - loss: 0.6293 - acc: 0.6668 - val_loss: 0.6292 - val_acc: 0.6667\n",
      "Epoch 18/100\n",
      "2307/2307 [==============================] - 1s 284us/step - loss: 0.6293 - acc: 0.6668 - val_loss: 0.6294 - val_acc: 0.6667\n",
      "Epoch 19/100\n",
      "2307/2307 [==============================] - 1s 268us/step - loss: 0.6293 - acc: 0.6668 - val_loss: 0.6291 - val_acc: 0.6667: 0s - loss: 0.6278 - acc: 0.6\n",
      "Epoch 20/100\n",
      "2307/2307 [==============================] - 1s 274us/step - loss: 0.6292 - acc: 0.6668 - val_loss: 0.6283 - val_acc: 0.6667\n",
      "Epoch 21/100\n",
      "2307/2307 [==============================] - 1s 271us/step - loss: 0.6300 - acc: 0.6668 - val_loss: 0.6284 - val_acc: 0.6667\n",
      "Epoch 22/100\n",
      "2307/2307 [==============================] - 1s 269us/step - loss: 0.6291 - acc: 0.6668 - val_loss: 0.6281 - val_acc: 0.6667\n",
      "Epoch 23/100\n",
      "2307/2307 [==============================] - 1s 368us/step - loss: 0.6294 - acc: 0.6668 - val_loss: 0.6290 - val_acc: 0.6667\n",
      "Epoch 24/100\n",
      "2307/2307 [==============================] - 1s 374us/step - loss: 0.6295 - acc: 0.6668 - val_loss: 0.6285 - val_acc: 0.6667\n",
      "Epoch 25/100\n",
      "2307/2307 [==============================] - 1s 271us/step - loss: 0.6291 - acc: 0.6668 - val_loss: 0.6289 - val_acc: 0.6667\n",
      "Epoch 26/100\n",
      "2307/2307 [==============================] - 1s 270us/step - loss: 0.6291 - acc: 0.6668 - val_loss: 0.6291 - val_acc: 0.6667\n",
      "Epoch 27/100\n",
      "2307/2307 [==============================] - 1s 278us/step - loss: 0.6293 - acc: 0.6668 - val_loss: 0.6286 - val_acc: 0.6667\n",
      "Epoch 28/100\n",
      "2307/2307 [==============================] - 1s 267us/step - loss: 0.6292 - acc: 0.6670 - val_loss: 0.6285 - val_acc: 0.6667\n",
      "Epoch 29/100\n",
      "2307/2307 [==============================] - 1s 271us/step - loss: 0.6290 - acc: 0.6668 - val_loss: 0.6292 - val_acc: 0.6667\n",
      "Epoch 30/100\n",
      "2307/2307 [==============================] - 1s 274us/step - loss: 0.6290 - acc: 0.6668 - val_loss: 0.6283 - val_acc: 0.6667\n",
      "Epoch 31/100\n",
      "2307/2307 [==============================] - 1s 268us/step - loss: 0.6291 - acc: 0.6670 - val_loss: 0.6288 - val_acc: 0.6667\n",
      "Epoch 32/100\n",
      "2307/2307 [==============================] - 1s 279us/step - loss: 0.6291 - acc: 0.6668 - val_loss: 0.6288 - val_acc: 0.6667\n",
      "Epoch 33/100\n",
      "2307/2307 [==============================] - 1s 271us/step - loss: 0.6290 - acc: 0.6670 - val_loss: 0.6289 - val_acc: 0.6667\n",
      "Epoch 34/100\n",
      "2307/2307 [==============================] - 1s 288us/step - loss: 0.6292 - acc: 0.6668 - val_loss: 0.6290 - val_acc: 0.6667\n",
      "Epoch 35/100\n",
      "2307/2307 [==============================] - 1s 479us/step - loss: 0.6291 - acc: 0.6665 - val_loss: 0.6286 - val_acc: 0.6667\n",
      "Epoch 36/100\n",
      "2307/2307 [==============================] - 1s 332us/step - loss: 0.6285 - acc: 0.6667 - val_loss: 0.6289 - val_acc: 0.6667\n",
      "Epoch 37/100\n",
      "2307/2307 [==============================] - 1s 361us/step - loss: 0.6290 - acc: 0.6670 - val_loss: 0.6285 - val_acc: 0.6667\n",
      "Epoch 38/100\n",
      "2307/2307 [==============================] - 1s 400us/step - loss: 0.6287 - acc: 0.6668 - val_loss: 0.6285 - val_acc: 0.6667\n",
      "Epoch 39/100\n",
      "2307/2307 [==============================] - 1s 413us/step - loss: 0.6288 - acc: 0.6670 - val_loss: 0.6293 - val_acc: 0.6667\n",
      "Epoch 40/100\n",
      "2307/2307 [==============================] - ETA: 0s - loss: 0.6289 - acc: 0.667 - 1s 407us/step - loss: 0.6288 - acc: 0.6672 - val_loss: 0.6290 - val_acc: 0.6667\n",
      "Epoch 41/100\n",
      "2307/2307 [==============================] - 1s 368us/step - loss: 0.6289 - acc: 0.6670 - val_loss: 0.6285 - val_acc: 0.6667\n",
      "Epoch 42/100\n",
      "2307/2307 [==============================] - 1s 334us/step - loss: 0.6290 - acc: 0.6674 - val_loss: 0.6286 - val_acc: 0.6667\n",
      "Epoch 43/100\n",
      "2307/2307 [==============================] - 1s 338us/step - loss: 0.6277 - acc: 0.6671 - val_loss: 0.6303 - val_acc: 0.6680\n",
      "Epoch 44/100\n",
      "2307/2307 [==============================] - 1s 331us/step - loss: 0.6287 - acc: 0.6671 - val_loss: 0.6281 - val_acc: 0.6667\n",
      "Epoch 45/100\n",
      "2307/2307 [==============================] - 1s 357us/step - loss: 0.6283 - acc: 0.6671 - val_loss: 0.6290 - val_acc: 0.6667\n",
      "Epoch 46/100\n",
      "2307/2307 [==============================] - 1s 339us/step - loss: 0.6282 - acc: 0.6665 - val_loss: 0.6285 - val_acc: 0.6667\n",
      "Epoch 47/100\n",
      "2307/2307 [==============================] - 1s 330us/step - loss: 0.6285 - acc: 0.6671 - val_loss: 0.6286 - val_acc: 0.6667\n",
      "Epoch 48/100\n",
      "2307/2307 [==============================] - 1s 339us/step - loss: 0.6286 - acc: 0.6672 - val_loss: 0.6281 - val_acc: 0.6680\n",
      "Epoch 49/100\n",
      "2307/2307 [==============================] - 1s 334us/step - loss: 0.6281 - acc: 0.6672 - val_loss: 0.6280 - val_acc: 0.6680\n",
      "Epoch 50/100\n",
      "2307/2307 [==============================] - 1s 348us/step - loss: 0.6279 - acc: 0.6671 - val_loss: 0.6303 - val_acc: 0.6667\n",
      "Epoch 51/100\n",
      "2307/2307 [==============================] - 1s 339us/step - loss: 0.6284 - acc: 0.6675 - val_loss: 0.6294 - val_acc: 0.6667\n",
      "Epoch 52/100\n",
      "2307/2307 [==============================] - 1s 342us/step - loss: 0.6280 - acc: 0.6667 - val_loss: 0.6297 - val_acc: 0.6667\n",
      "Epoch 53/100\n",
      "2307/2307 [==============================] - 1s 341us/step - loss: 0.6278 - acc: 0.6672 - val_loss: 0.6282 - val_acc: 0.6667\n",
      "Epoch 54/100\n",
      "2307/2307 [==============================] - 1s 359us/step - loss: 0.6280 - acc: 0.6675 - val_loss: 0.6300 - val_acc: 0.6667\n",
      "Epoch 55/100\n",
      "2307/2307 [==============================] - 1s 349us/step - loss: 0.6276 - acc: 0.6674 - val_loss: 0.6282 - val_acc: 0.6680\n",
      "Epoch 56/100\n",
      "2307/2307 [==============================] - 1s 343us/step - loss: 0.6280 - acc: 0.6667 - val_loss: 0.6305 - val_acc: 0.6680\n",
      "Epoch 57/100\n",
      "2307/2307 [==============================] - 1s 340us/step - loss: 0.6276 - acc: 0.6675 - val_loss: 0.6285 - val_acc: 0.6667\n",
      "Epoch 58/100\n",
      "2307/2307 [==============================] - 1s 337us/step - loss: 0.6280 - acc: 0.6671 - val_loss: 0.6298 - val_acc: 0.6667\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2307/2307 [==============================] - 1s 315us/step - loss: 0.6280 - acc: 0.6671 - val_loss: 0.6300 - val_acc: 0.6693\n",
      "Epoch 60/100\n",
      "2307/2307 [==============================] - 1s 338us/step - loss: 0.6281 - acc: 0.6677 - val_loss: 0.6289 - val_acc: 0.6667\n",
      "Epoch 61/100\n",
      "2307/2307 [==============================] - 1s 362us/step - loss: 0.6272 - acc: 0.6670 - val_loss: 0.6301 - val_acc: 0.6667\n",
      "Epoch 62/100\n",
      "2307/2307 [==============================] - 1s 351us/step - loss: 0.6276 - acc: 0.6675 - val_loss: 0.6286 - val_acc: 0.6654\n",
      "Epoch 63/100\n",
      "2307/2307 [==============================] - 1s 316us/step - loss: 0.6273 - acc: 0.6674 - val_loss: 0.6288 - val_acc: 0.6654\n",
      "Epoch 64/100\n",
      "2307/2307 [==============================] - 1s 319us/step - loss: 0.6273 - acc: 0.6672 - val_loss: 0.6302 - val_acc: 0.6615\n",
      "Epoch 65/100\n",
      "2307/2307 [==============================] - 1s 304us/step - loss: 0.6270 - acc: 0.6680 - val_loss: 0.6289 - val_acc: 0.6654\n",
      "Epoch 66/100\n",
      "2307/2307 [==============================] - 1s 304us/step - loss: 0.6275 - acc: 0.6670 - val_loss: 0.6283 - val_acc: 0.6628\n",
      "Epoch 67/100\n",
      "2307/2307 [==============================] - 1s 310us/step - loss: 0.6268 - acc: 0.6678 - val_loss: 0.6293 - val_acc: 0.6667\n",
      "Epoch 68/100\n",
      "2307/2307 [==============================] - 1s 308us/step - loss: 0.6269 - acc: 0.6677 - val_loss: 0.6281 - val_acc: 0.6654\n",
      "Epoch 69/100\n",
      "2307/2307 [==============================] - 1s 308us/step - loss: 0.6271 - acc: 0.6668 - val_loss: 0.6282 - val_acc: 0.6667\n",
      "Epoch 70/100\n",
      "2307/2307 [==============================] - 1s 317us/step - loss: 0.6264 - acc: 0.6678 - val_loss: 0.6289 - val_acc: 0.6654\n",
      "Epoch 71/100\n",
      "2307/2307 [==============================] - 1s 314us/step - loss: 0.6267 - acc: 0.6671 - val_loss: 0.6292 - val_acc: 0.6667\n",
      "Epoch 72/100\n",
      "2307/2307 [==============================] - 1s 322us/step - loss: 0.6261 - acc: 0.6675 - val_loss: 0.6281 - val_acc: 0.6667\n",
      "Epoch 73/100\n",
      "2307/2307 [==============================] - 1s 376us/step - loss: 0.6259 - acc: 0.6691 - val_loss: 0.6304 - val_acc: 0.6667\n",
      "Epoch 74/100\n",
      "2307/2307 [==============================] - 1s 375us/step - loss: 0.6260 - acc: 0.6683 - val_loss: 0.6297 - val_acc: 0.6628\n",
      "Epoch 75/100\n",
      "2307/2307 [==============================] - 1s 333us/step - loss: 0.6258 - acc: 0.6690 - val_loss: 0.6283 - val_acc: 0.6641\n",
      "Epoch 76/100\n",
      "2307/2307 [==============================] - 1s 345us/step - loss: 0.6259 - acc: 0.6671 - val_loss: 0.6283 - val_acc: 0.6641\n",
      "Epoch 77/100\n",
      "2307/2307 [==============================] - 1s 359us/step - loss: 0.6257 - acc: 0.6672 - val_loss: 0.6298 - val_acc: 0.6654\n",
      "Epoch 78/100\n",
      "2307/2307 [==============================] - 1s 373us/step - loss: 0.6256 - acc: 0.6677 - val_loss: 0.6292 - val_acc: 0.6641\n",
      "Epoch 79/100\n",
      "2307/2307 [==============================] - 1s 387us/step - loss: 0.6253 - acc: 0.6690 - val_loss: 0.6296 - val_acc: 0.6628\n",
      "Epoch 80/100\n",
      "2307/2307 [==============================] - 1s 361us/step - loss: 0.6248 - acc: 0.6685 - val_loss: 0.6303 - val_acc: 0.6641\n",
      "Epoch 81/100\n",
      "2307/2307 [==============================] - 1s 356us/step - loss: 0.6259 - acc: 0.6683 - val_loss: 0.6268 - val_acc: 0.6667\n",
      "Epoch 82/100\n",
      "2307/2307 [==============================] - 1s 377us/step - loss: 0.6258 - acc: 0.6675 - val_loss: 0.6285 - val_acc: 0.6628\n",
      "Epoch 83/100\n",
      "2307/2307 [==============================] - 1s 388us/step - loss: 0.6255 - acc: 0.6665 - val_loss: 0.6286 - val_acc: 0.6667\n",
      "Epoch 84/100\n",
      "2307/2307 [==============================] - 1s 385us/step - loss: 0.6249 - acc: 0.6691 - val_loss: 0.6280 - val_acc: 0.6628\n",
      "Epoch 85/100\n",
      "2307/2307 [==============================] - 1s 351us/step - loss: 0.6244 - acc: 0.6690 - val_loss: 0.6284 - val_acc: 0.6680\n",
      "Epoch 86/100\n",
      "2307/2307 [==============================] - 1s 414us/step - loss: 0.6239 - acc: 0.6700 - val_loss: 0.6330 - val_acc: 0.6628\n",
      "Epoch 87/100\n",
      "2307/2307 [==============================] - 1s 420us/step - loss: 0.6241 - acc: 0.6693 - val_loss: 0.6311 - val_acc: 0.6628\n",
      "Epoch 88/100\n",
      "2307/2307 [==============================] - 1s 388us/step - loss: 0.6240 - acc: 0.6693 - val_loss: 0.6289 - val_acc: 0.6628\n",
      "Epoch 89/100\n",
      "2307/2307 [==============================] - 1s 385us/step - loss: 0.6240 - acc: 0.6677 - val_loss: 0.6285 - val_acc: 0.6667\n",
      "Epoch 90/100\n",
      "2307/2307 [==============================] - 1s 387us/step - loss: 0.6238 - acc: 0.6681 - val_loss: 0.6320 - val_acc: 0.6667\n",
      "Epoch 91/100\n",
      "2307/2307 [==============================] - 1s 336us/step - loss: 0.6241 - acc: 0.6678 - val_loss: 0.6291 - val_acc: 0.6641\n",
      "Epoch 92/100\n",
      "2307/2307 [==============================] - 1s 320us/step - loss: 0.6240 - acc: 0.6671 - val_loss: 0.6305 - val_acc: 0.6654\n",
      "Epoch 93/100\n",
      "2307/2307 [==============================] - 1s 314us/step - loss: 0.6238 - acc: 0.6687 - val_loss: 0.6308 - val_acc: 0.6641\n",
      "Epoch 94/100\n",
      "2307/2307 [==============================] - 1s 319us/step - loss: 0.6254 - acc: 0.6698 - val_loss: 0.6298 - val_acc: 0.6628\n",
      "Epoch 95/100\n",
      "2307/2307 [==============================] - 1s 326us/step - loss: 0.6244 - acc: 0.6690 - val_loss: 0.6317 - val_acc: 0.6615\n",
      "Epoch 96/100\n",
      "2307/2307 [==============================] - 1s 326us/step - loss: 0.6224 - acc: 0.6681 - val_loss: 0.6307 - val_acc: 0.6628\n",
      "Epoch 97/100\n",
      "2307/2307 [==============================] - 1s 322us/step - loss: 0.6229 - acc: 0.6697 - val_loss: 0.6291 - val_acc: 0.6719\n",
      "Epoch 98/100\n",
      "2307/2307 [==============================] - 1s 329us/step - loss: 0.6238 - acc: 0.6670 - val_loss: 0.6301 - val_acc: 0.6654\n",
      "Epoch 99/100\n",
      "2307/2307 [==============================] - 1s 338us/step - loss: 0.6241 - acc: 0.6687 - val_loss: 0.6320 - val_acc: 0.6654\n",
      "Epoch 100/100\n",
      "2307/2307 [==============================] - 1s 343us/step - loss: 0.6232 - acc: 0.6703 - val_loss: 0.6304 - val_acc: 0.6654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2c460fd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Dense(units = 15, kernel_initializer='uniform', activation = 'relu'))\n",
    "classifier.add(Dense(units = 15, kernel_initializer='uniform', activation = 'relu'))\n",
    "classifier.add(Dense(units = 3, kernel_initializer='uniform', activation = 'softmax'))\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1a2cf41150>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for no tweet model: 0.64\n",
      "acc for no tweet model: 0.66\n"
     ]
    }
   ],
   "source": [
    "score_no_tweet, acc_no_tweet = classifier.evaluate(X_test, y_test, verbose = 2, batch_size = 32)\n",
    "print(\"score for no tweet model: %.2f\" % (score_no_tweet))\n",
    "print(\"acc for no tweet model: %.2f\" % (acc_no_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network for the non-tweet data has a 66% accuracy rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Model with tweets included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, tokenize the tweets to feed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=15000)\n",
    "tokenizer.fit_on_texts(list(df.tweet))\n",
    "list_tokenized_tweets = tokenizer.texts_to_sequences(df.tweet)\n",
    "X_t = sequence.pad_sequences(list_tokenized_tweets) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(3205, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0, 1328,  711,    4,    2,   16,    6,    3, 1633, 1329,\n",
       "          12,  351,   36,    3, 2220,    2],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    5,\n",
       "         712,    4, 1634,  156,    2,  151,  394,    9,  461,    2,    1,\n",
       "           9,  127,    2,    1,  276,   47],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,  505]], dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(X_t))\n",
    "print(X_t.shape)\n",
    "X_t[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(3205, 42)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.71000000e+02,  1.05000000e+02,  1.00000000e+00,\n",
       "         3.66666667e-01, -3.17503779e-02, -2.88503208e-02,\n",
       "        -3.02899383e-02, -1.87361455e-02, -6.06745282e-02,\n",
       "         5.77981532e-02, -1.24514649e-01,  1.16679040e-01,\n",
       "         5.94730409e-02, -2.91580810e-02, -4.82227257e-02,\n",
       "         7.24178428e-02,  3.90785157e-02,  2.93682030e-02,\n",
       "        -5.01187584e-02,  1.81687862e-02,  1.42445412e-02,\n",
       "        -3.82909349e-02,  9.11399155e-03, -3.36043675e-02,\n",
       "         3.00319803e-03,  1.42111843e-02,  1.02493801e-02,\n",
       "         1.01547097e-02,  3.73028256e-02, -1.27392087e-02,\n",
       "         3.30800335e-03, -4.88425818e-02, -2.72917718e-03,\n",
       "         1.78852054e-02,  4.58221637e-02, -6.86176435e-03,\n",
       "        -1.29544357e-02, -5.46828939e-03, -1.00572163e-01,\n",
       "        -2.93740249e-02,  4.13625539e-02,  3.59321462e-02],\n",
       "       [ 7.70000000e+01,  1.13000000e+02,  1.00000000e+00,\n",
       "         2.00000000e-01, -5.93306206e-02, -9.98259555e-02,\n",
       "        -1.54772144e-01, -4.03554083e-02,  7.22851718e-02,\n",
       "         2.09163426e-01, -8.00307351e-02,  7.77922425e-02,\n",
       "        -7.33890794e-02, -8.92879922e-02,  3.86667278e-02,\n",
       "         6.68178386e-02,  5.67030796e-03,  1.43317685e-02,\n",
       "        -1.94389487e-02,  5.42790281e-02, -1.06515944e-01,\n",
       "         2.14890229e-01,  3.64107528e-02,  1.52512417e-03,\n",
       "         5.25963522e-02,  1.93625545e-02, -9.19316254e-02,\n",
       "        -3.80315566e-02,  1.43679493e-02, -4.18951930e-03,\n",
       "         6.96910140e-04, -9.49459749e-02, -5.93606627e-02,\n",
       "        -1.16671812e-02,  3.88158805e-03,  1.80024713e-02,\n",
       "        -1.98131687e-02, -3.58088107e-02, -3.40787869e-02,\n",
       "        -1.46867250e-02, -8.98559724e-03, -1.98452008e-02],\n",
       "       [ 1.71000000e+02,  6.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.33026683e-03,  3.39832204e-02,\n",
       "         6.95311431e-02, -1.41766485e-02, -3.77738510e-02,\n",
       "        -7.12793800e-02,  7.40175321e-03, -1.59693766e-02,\n",
       "        -3.35558882e-02,  7.77574928e-03,  1.96136173e-02,\n",
       "        -2.08019085e-02, -1.58564633e-02,  1.97917363e-02,\n",
       "        -1.23597230e-02, -2.78961399e-02, -5.84956460e-02,\n",
       "        -6.93341779e-03,  5.02067339e-02, -1.71281510e-02,\n",
       "         1.14986700e-02, -3.37025366e-02,  7.06610895e-03,\n",
       "         5.05696135e-02, -4.76532783e-02, -5.02923896e-02,\n",
       "        -1.16384023e-02, -1.04160448e-02,  8.79705537e-02,\n",
       "         1.57217677e-02, -2.57264842e-02,  6.57348170e-02,\n",
       "         4.16859678e-02, -2.17508633e-02, -1.41376994e-02,\n",
       "        -3.70467810e-02, -5.00306913e-02, -8.47224322e-02]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(X))\n",
    "print(X.shape)\n",
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_try = np.append(X_t, X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3205, 70)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_try.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  1.32800000e+03,  7.11000000e+02,  4.00000000e+00,\n",
       "        2.00000000e+00,  1.60000000e+01,  6.00000000e+00,  3.00000000e+00,\n",
       "        1.63300000e+03,  1.32900000e+03,  1.20000000e+01,  3.51000000e+02,\n",
       "        3.60000000e+01,  3.00000000e+00,  2.22000000e+03,  2.00000000e+00,\n",
       "        1.71000000e+02,  1.05000000e+02,  1.00000000e+00,  3.66666667e-01,\n",
       "       -3.17503779e-02, -2.88503208e-02, -3.02899383e-02, -1.87361455e-02,\n",
       "       -6.06745282e-02,  5.77981532e-02, -1.24514649e-01,  1.16679040e-01,\n",
       "        5.94730409e-02, -2.91580810e-02, -4.82227257e-02,  7.24178428e-02,\n",
       "        3.90785157e-02,  2.93682030e-02, -5.01187584e-02,  1.81687862e-02,\n",
       "        1.42445412e-02, -3.82909349e-02,  9.11399155e-03, -3.36043675e-02,\n",
       "        3.00319803e-03,  1.42111843e-02,  1.02493801e-02,  1.01547097e-02,\n",
       "        3.73028256e-02, -1.27392087e-02,  3.30800335e-03, -4.88425818e-02,\n",
       "       -2.72917718e-03,  1.78852054e-02,  4.58221637e-02, -6.86176435e-03,\n",
       "       -1.29544357e-02, -5.46828939e-03, -1.00572163e-01, -2.93740249e-02,\n",
       "        4.13625539e-02,  3.59321462e-02])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use X_try because it combines tweets vectors and pca vectors\n",
    "X_try[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tweet, X_test_tweet, y_train_tweet, y_test_tweet = train_test_split(X_try, y, test_size = 0.2, random_state = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.662187561799708,\n",
       "  0.664065903097094,\n",
       "  0.6681115638398484,\n",
       "  0.6692674673890261,\n",
       "  0.6691229788317637,\n",
       "  0.6720127371879796,\n",
       "  0.6741800526287011,\n",
       "  0.6730241489503411,\n",
       "  0.6801040531186557,\n",
       "  0.6848721507053434,\n",
       "  0.6880508829465303,\n",
       "  0.6886288340752085,\n",
       "  0.6915185911396028,\n",
       "  0.6907961532622123,\n",
       "  0.6948418118088702,\n",
       "  0.6916630800844117,\n",
       "  0.694119372691331,\n",
       "  0.6965756628696258,\n",
       "  0.7007658081649959,\n",
       "  0.7071232722598234,\n",
       "  0.7029331272228176,\n",
       "  0.7032221017536995,\n",
       "  0.7081346909980212,\n",
       "  0.7042335153246112,\n",
       "  0.7100130297117643,\n",
       "  0.7100130275156676,\n",
       "  0.7084236634619886,\n",
       "  0.714492148504559,\n",
       "  0.7170929284544285,\n",
       "  0.7129027840633334,\n",
       "  0.7124693209751891,\n",
       "  0.7204161504155172,\n",
       "  0.7155035653050245,\n",
       "  0.7160815134625131,\n",
       "  0.7147811263941768,\n",
       "  0.7191157619261772,\n",
       "  0.7221500047058268,\n",
       "  0.7195492201053998,\n",
       "  0.7238838577043147,\n",
       "  0.7257621968056043,\n",
       "  0.7266291265989931,\n",
       "  0.7251842452893796,\n",
       "  0.7240283447630642,\n",
       "  0.7290854183791466,\n",
       "  0.7274960503208209,\n",
       "  0.7270625891704088,\n",
       "  0.726340151034654,\n",
       "  0.7257622004227045,\n",
       "  0.7259066858795953,\n",
       "  0.7300968328543334,\n",
       "  0.7233059059297258,\n",
       "  0.7221500050158639,\n",
       "  0.7303858072560331,\n",
       "  0.7329865883685419,\n",
       "  0.732986586818356,\n",
       "  0.7309637585138934,\n",
       "  0.7309637578679826,\n",
       "  0.7344314642525049,\n",
       "  0.736165318930361,\n",
       "  0.7300968327251512,\n",
       "  0.7287964411871124,\n",
       "  0.7344314664486016,\n",
       "  0.729663367182546,\n",
       "  0.7306747830787365,\n",
       "  0.7344314654151444,\n",
       "  0.7315417096425715,\n",
       "  0.7272070769525784,\n",
       "  0.7361653176385394,\n",
       "  0.7352983884910613,\n",
       "  0.7341424919177197,\n",
       "  0.7308192694399023,\n",
       "  0.7371767315969976,\n",
       "  0.7283629785640239,\n",
       "  0.732986587593449,\n",
       "  0.7344314694197911,\n",
       "  0.7371767317261797,\n",
       "  0.7386216137333768,\n",
       "  0.738188148397463,\n",
       "  0.7394885388245354,\n",
       "  0.7383326363604876,\n",
       "  0.7393440524633694,\n",
       "  0.7377546869886867,\n",
       "  0.739055076640666,\n",
       "  0.7374657114243477,\n",
       "  0.7298078569024479,\n",
       "  0.7383326378590005,\n",
       "  0.7368877561618407,\n",
       "  0.7381881498184667,\n",
       "  0.7404999536872685,\n",
       "  0.7419448284085922,\n",
       "  0.735298391591433,\n",
       "  0.732986589272817,\n",
       "  0.7374657096157975,\n",
       "  0.7378991724455776,\n",
       "  0.7364542969491609,\n",
       "  0.7360208333959608,\n",
       "  0.7396330228604224,\n",
       "  0.7419448334466962,\n",
       "  0.7438231717728928,\n",
       "  0.745123562975058],\n",
       " 'loss': [0.63817344690537,\n",
       "  0.630789007776787,\n",
       "  0.6267670243397181,\n",
       "  0.6244728732377769,\n",
       "  0.623332778276899,\n",
       "  0.6210136049538916,\n",
       "  0.6182410826298599,\n",
       "  0.6162671136504704,\n",
       "  0.609468026857744,\n",
       "  0.6062663201909588,\n",
       "  0.6016782277013815,\n",
       "  0.5963832641453984,\n",
       "  0.5948804670409511,\n",
       "  0.5917337821787655,\n",
       "  0.5914882517088322,\n",
       "  0.5893165890303431,\n",
       "  0.5834852795891292,\n",
       "  0.584850863455486,\n",
       "  0.5745247907074282,\n",
       "  0.5696162311711453,\n",
       "  0.5741801147704834,\n",
       "  0.5720247132828807,\n",
       "  0.5704689821382596,\n",
       "  0.5724900181178774,\n",
       "  0.5663280847607932,\n",
       "  0.5662651401244049,\n",
       "  0.5601971712678762,\n",
       "  0.5600701234120541,\n",
       "  0.5566778656823531,\n",
       "  0.5574729185265589,\n",
       "  0.5609853876117612,\n",
       "  0.5512231242930574,\n",
       "  0.5517987701662391,\n",
       "  0.5532177605823257,\n",
       "  0.5545657499042176,\n",
       "  0.5485981946183739,\n",
       "  0.5488851079188439,\n",
       "  0.5513551495371828,\n",
       "  0.5451650806045862,\n",
       "  0.5421081112998877,\n",
       "  0.5412777620680569,\n",
       "  0.5421887328731172,\n",
       "  0.5413924412108315,\n",
       "  0.5378778939336009,\n",
       "  0.5391086591339028,\n",
       "  0.5360646454555212,\n",
       "  0.5336713576967912,\n",
       "  0.5336555556580262,\n",
       "  0.537123242613553,\n",
       "  0.5350393946934532,\n",
       "  0.5393509796082224,\n",
       "  0.5424026959015789,\n",
       "  0.5316999180135037,\n",
       "  0.5262348616490077,\n",
       "  0.5280922419408931,\n",
       "  0.5323837730705764,\n",
       "  0.5257912768292438,\n",
       "  0.5243194773179606,\n",
       "  0.5243428175615757,\n",
       "  0.5271342001438761,\n",
       "  0.5279488341085933,\n",
       "  0.5211369547690982,\n",
       "  0.5365813330080688,\n",
       "  0.5291463832189678,\n",
       "  0.52327150331207,\n",
       "  0.5310539466300257,\n",
       "  0.527684517569701,\n",
       "  0.5163441830555702,\n",
       "  0.5199132222709689,\n",
       "  0.5213929145791906,\n",
       "  0.5243068736354976,\n",
       "  0.5184426015984924,\n",
       "  0.5287310240275942,\n",
       "  0.5235826042910701,\n",
       "  0.5192730227718635,\n",
       "  0.5203494281287208,\n",
       "  0.5114287969143193,\n",
       "  0.5117687590074684,\n",
       "  0.5176950663418598,\n",
       "  0.5184050631512798,\n",
       "  0.5167429986638737,\n",
       "  0.5113779937983283,\n",
       "  0.517855683600794,\n",
       "  0.521844722824692,\n",
       "  0.525961510381711,\n",
       "  0.5186400865133985,\n",
       "  0.5178495802608568,\n",
       "  0.5143451677109703,\n",
       "  0.5129547373629464,\n",
       "  0.5042002120341478,\n",
       "  0.5132670844588179,\n",
       "  0.5216175602610926,\n",
       "  0.509966673727515,\n",
       "  0.5081001665228416,\n",
       "  0.5136303638269956,\n",
       "  0.5201785523471866,\n",
       "  0.5081518203291379,\n",
       "  0.5036217916513037,\n",
       "  0.5004797494535483,\n",
       "  0.4986154320026095],\n",
       " 'val_acc': [0.6653696685913472,\n",
       "  0.6601815944979627,\n",
       "  0.6640726506478127,\n",
       "  0.6640726483285659,\n",
       "  0.658884581192922,\n",
       "  0.6640726506478127,\n",
       "  0.6485084492408811,\n",
       "  0.6575875597705173,\n",
       "  0.6653696755490878,\n",
       "  0.6523995077099781,\n",
       "  0.6562905522635939,\n",
       "  0.6549935343200595,\n",
       "  0.6640726529670596,\n",
       "  0.656290549944347,\n",
       "  0.6536965233342656,\n",
       "  0.6511024804894562,\n",
       "  0.6562905545828407,\n",
       "  0.6459144179923061,\n",
       "  0.6368352923875653,\n",
       "  0.6355382848806418,\n",
       "  0.6433203763071201,\n",
       "  0.6459144133538124,\n",
       "  0.635538290678759,\n",
       "  0.648508453879375,\n",
       "  0.65110248512795,\n",
       "  0.6523995042311078,\n",
       "  0.6420233664809498,\n",
       "  0.6459144133538124,\n",
       "  0.6407263520162857,\n",
       "  0.6381323172888403,\n",
       "  0.635538290678759,\n",
       "  0.6173800417885242,\n",
       "  0.6368353039837997,\n",
       "  0.6342412646178606,\n",
       "  0.6316472356885324,\n",
       "  0.6199740753563462,\n",
       "  0.6225681205204024,\n",
       "  0.6394293375516216,\n",
       "  0.625162152928601,\n",
       "  0.6238651315061963,\n",
       "  0.6290531963225933,\n",
       "  0.6342412727352246,\n",
       "  0.6342412680967309,\n",
       "  0.6290531998014636,\n",
       "  0.6225681135626618,\n",
       "  0.629053200961087,\n",
       "  0.6264591557970307,\n",
       "  0.6316472310500386,\n",
       "  0.6251621471304838,\n",
       "  0.6355382814017715,\n",
       "  0.6225681182011557,\n",
       "  0.6303502189046214,\n",
       "  0.6277561830175526,\n",
       "  0.6186770666897993,\n",
       "  0.632944257110937,\n",
       "  0.6316472345289089,\n",
       "  0.6134889960752851,\n",
       "  0.6342412611389903,\n",
       "  0.6433203960207186,\n",
       "  0.6264591662336416,\n",
       "  0.6290531986418402,\n",
       "  0.6173800603424993,\n",
       "  0.6121919862491148,\n",
       "  0.6407263485374154,\n",
       "  0.643320386743731,\n",
       "  0.6420233711194435,\n",
       "  0.6316472345289089,\n",
       "  0.630350217744998,\n",
       "  0.6316472275711683,\n",
       "  0.6407263531759091,\n",
       "  0.6407263531759091,\n",
       "  0.6264591604355244,\n",
       "  0.6264591557970307,\n",
       "  0.6407263438989216,\n",
       "  0.6186770678494227,\n",
       "  0.6212710909806337,\n",
       "  0.6238651303465729,\n",
       "  0.6355382790825246,\n",
       "  0.6199740765159696,\n",
       "  0.6342412634582371,\n",
       "  0.6108949659863334,\n",
       "  0.6199740834737102,\n",
       "  0.6225681112434149,\n",
       "  0.6251621482901072,\n",
       "  0.6173800487462648,\n",
       "  0.6355382790825246,\n",
       "  0.6199740834737102,\n",
       "  0.6290531963225933,\n",
       "  0.6134889949156617,\n",
       "  0.6251621517689775,\n",
       "  0.6160830354412242,\n",
       "  0.626459167393265,\n",
       "  0.6303502189046214,\n",
       "  0.6173800464270179,\n",
       "  0.6251621471304838,\n",
       "  0.6368353063030465,\n",
       "  0.612191981610621,\n",
       "  0.6134889960752851,\n",
       "  0.6277561806983057,\n",
       "  0.6290531986418402],\n",
       " 'val_loss': [0.6337751634853823,\n",
       "  0.6368494963831475,\n",
       "  0.6313727050439857,\n",
       "  0.6422065865204956,\n",
       "  0.6360280038317818,\n",
       "  0.6317550693040692,\n",
       "  0.6481079380335975,\n",
       "  0.6398960535164473,\n",
       "  0.6304831987225128,\n",
       "  0.6449755441817792,\n",
       "  0.6462871372003963,\n",
       "  0.6538759279807718,\n",
       "  0.6436505853434017,\n",
       "  0.6684376351564311,\n",
       "  0.6475104501739086,\n",
       "  0.6555000768561308,\n",
       "  0.6647214840822183,\n",
       "  0.6955652475820905,\n",
       "  0.6932870826832516,\n",
       "  0.697369881873001,\n",
       "  0.7135605007294087,\n",
       "  0.6861667505498062,\n",
       "  0.7236714627492289,\n",
       "  0.7052563932155357,\n",
       "  0.7233027852926737,\n",
       "  0.7143737300360713,\n",
       "  0.7464521102868166,\n",
       "  0.7263236268485103,\n",
       "  0.7258196487037124,\n",
       "  0.7323685061143066,\n",
       "  0.7603276770866335,\n",
       "  0.78497034728759,\n",
       "  0.7704148960484605,\n",
       "  0.7949994911479579,\n",
       "  0.8007083460978497,\n",
       "  0.824546167358814,\n",
       "  0.7818850415689936,\n",
       "  0.7698899642038902,\n",
       "  0.7910514594515938,\n",
       "  0.8165716881418043,\n",
       "  0.8150945683861521,\n",
       "  0.8092655882761173,\n",
       "  0.8124351911971541,\n",
       "  0.8396444717269927,\n",
       "  0.8103761798212964,\n",
       "  0.8222679001811878,\n",
       "  0.8402311769440944,\n",
       "  0.8481378740837602,\n",
       "  0.8406145825460263,\n",
       "  0.8410305172089009,\n",
       "  0.8909988670033704,\n",
       "  0.8387885058900262,\n",
       "  0.8396151801027677,\n",
       "  0.8352427981243059,\n",
       "  0.8534340204431853,\n",
       "  0.8718105946533411,\n",
       "  0.884317389258151,\n",
       "  0.8372441001439372,\n",
       "  0.8391761631353356,\n",
       "  0.8840397086124939,\n",
       "  0.8735054696580316,\n",
       "  0.9241489659023656,\n",
       "  0.9130598575688522,\n",
       "  0.86253925496966,\n",
       "  0.810121553185385,\n",
       "  0.8537117441108718,\n",
       "  0.8980360170282743,\n",
       "  0.9022430510836352,\n",
       "  0.8558253321202348,\n",
       "  0.8738701051311271,\n",
       "  0.8778704657628842,\n",
       "  0.9575582318268862,\n",
       "  0.9051770628194401,\n",
       "  0.9264556246045035,\n",
       "  0.9436104167760114,\n",
       "  0.9404912208304795,\n",
       "  0.9480144010443632,\n",
       "  1.0001373068367925,\n",
       "  1.0212162791059174,\n",
       "  0.9970332226400709,\n",
       "  1.0161266507805553,\n",
       "  0.9443979956760481,\n",
       "  1.0394208303685317,\n",
       "  0.8786006736848141,\n",
       "  0.9609226858105641,\n",
       "  0.9246675907869747,\n",
       "  0.9778858073026754,\n",
       "  1.0109676725669594,\n",
       "  1.0002346532817945,\n",
       "  1.0519206206158442,\n",
       "  1.0646376050863748,\n",
       "  1.0269238909394824,\n",
       "  0.9361398992371467,\n",
       "  0.9983475141024312,\n",
       "  1.0207179156258877,\n",
       "  1.0040513694518272,\n",
       "  1.0212626777270424,\n",
       "  1.0413976068162734,\n",
       "  1.0152688058897679,\n",
       "  1.0524672940083515]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2307 samples, validate on 257 samples\n",
      "Epoch 1/100\n",
      "2307/2307 [==============================] - 2s 656us/step - loss: 0.6382 - acc: 0.6622 - val_loss: 0.6338 - val_acc: 0.6654\n",
      "Epoch 2/100\n",
      "2307/2307 [==============================] - 1s 329us/step - loss: 0.6308 - acc: 0.6641 - val_loss: 0.6368 - val_acc: 0.6602\n",
      "Epoch 3/100\n",
      "2307/2307 [==============================] - 1s 332us/step - loss: 0.6268 - acc: 0.6681 - val_loss: 0.6314 - val_acc: 0.6641\n",
      "Epoch 4/100\n",
      "2307/2307 [==============================] - 1s 338us/step - loss: 0.6245 - acc: 0.6693 - val_loss: 0.6422 - val_acc: 0.6641\n",
      "Epoch 5/100\n",
      "2307/2307 [==============================] - 1s 334us/step - loss: 0.6233 - acc: 0.6691 - val_loss: 0.6360 - val_acc: 0.6589\n",
      "Epoch 6/100\n",
      "2307/2307 [==============================] - 1s 341us/step - loss: 0.6210 - acc: 0.6720 - val_loss: 0.6318 - val_acc: 0.6641\n",
      "Epoch 7/100\n",
      "2307/2307 [==============================] - 1s 361us/step - loss: 0.6182 - acc: 0.6742 - val_loss: 0.6481 - val_acc: 0.6485\n",
      "Epoch 8/100\n",
      "2307/2307 [==============================] - 1s 363us/step - loss: 0.6163 - acc: 0.6730 - val_loss: 0.6399 - val_acc: 0.6576\n",
      "Epoch 9/100\n",
      "2307/2307 [==============================] - 1s 342us/step - loss: 0.6095 - acc: 0.6801 - val_loss: 0.6305 - val_acc: 0.6654\n",
      "Epoch 10/100\n",
      "2307/2307 [==============================] - 1s 343us/step - loss: 0.6063 - acc: 0.6849 - val_loss: 0.6450 - val_acc: 0.6524\n",
      "Epoch 11/100\n",
      "2307/2307 [==============================] - 1s 340us/step - loss: 0.6017 - acc: 0.6881 - val_loss: 0.6463 - val_acc: 0.6563\n",
      "Epoch 12/100\n",
      "2307/2307 [==============================] - 1s 363us/step - loss: 0.5964 - acc: 0.6886 - val_loss: 0.6539 - val_acc: 0.6550\n",
      "Epoch 13/100\n",
      "2307/2307 [==============================] - 1s 336us/step - loss: 0.5949 - acc: 0.6915 - val_loss: 0.6437 - val_acc: 0.6641\n",
      "Epoch 14/100\n",
      "2307/2307 [==============================] - 1s 341us/step - loss: 0.5917 - acc: 0.6908 - val_loss: 0.6684 - val_acc: 0.6563\n",
      "Epoch 15/100\n",
      "2307/2307 [==============================] - 1s 350us/step - loss: 0.5915 - acc: 0.6948 - val_loss: 0.6475 - val_acc: 0.6537\n",
      "Epoch 16/100\n",
      "2307/2307 [==============================] - 1s 341us/step - loss: 0.5893 - acc: 0.6917 - val_loss: 0.6555 - val_acc: 0.6511\n",
      "Epoch 17/100\n",
      "2307/2307 [==============================] - 1s 335us/step - loss: 0.5835 - acc: 0.6941 - val_loss: 0.6647 - val_acc: 0.6563\n",
      "Epoch 18/100\n",
      "2307/2307 [==============================] - 1s 337us/step - loss: 0.5849 - acc: 0.6966 - val_loss: 0.6956 - val_acc: 0.6459\n",
      "Epoch 19/100\n",
      "2307/2307 [==============================] - 1s 362us/step - loss: 0.5745 - acc: 0.7008 - val_loss: 0.6933 - val_acc: 0.6368\n",
      "Epoch 20/100\n",
      "2307/2307 [==============================] - 1s 478us/step - loss: 0.5696 - acc: 0.7071 - val_loss: 0.6974 - val_acc: 0.6355\n",
      "Epoch 21/100\n",
      "2307/2307 [==============================] - 1s 373us/step - loss: 0.5742 - acc: 0.7029 - val_loss: 0.7136 - val_acc: 0.6433\n",
      "Epoch 22/100\n",
      "2307/2307 [==============================] - 1s 291us/step - loss: 0.5720 - acc: 0.7032 - val_loss: 0.6862 - val_acc: 0.6459\n",
      "Epoch 23/100\n",
      "2307/2307 [==============================] - 1s 340us/step - loss: 0.5705 - acc: 0.7081 - val_loss: 0.7237 - val_acc: 0.6355\n",
      "Epoch 24/100\n",
      "2307/2307 [==============================] - 1s 325us/step - loss: 0.5725 - acc: 0.7042 - val_loss: 0.7053 - val_acc: 0.6485\n",
      "Epoch 25/100\n",
      "2307/2307 [==============================] - 1s 363us/step - loss: 0.5663 - acc: 0.7100 - val_loss: 0.7233 - val_acc: 0.6511\n",
      "Epoch 26/100\n",
      "2307/2307 [==============================] - 1s 329us/step - loss: 0.5663 - acc: 0.7100 - val_loss: 0.7144 - val_acc: 0.6524\n",
      "Epoch 27/100\n",
      "2307/2307 [==============================] - 1s 303us/step - loss: 0.5602 - acc: 0.7084 - val_loss: 0.7465 - val_acc: 0.6420\n",
      "Epoch 28/100\n",
      "2307/2307 [==============================] - 1s 463us/step - loss: 0.5601 - acc: 0.7145 - val_loss: 0.7263 - val_acc: 0.6459\n",
      "Epoch 29/100\n",
      "2307/2307 [==============================] - 1s 433us/step - loss: 0.5567 - acc: 0.7171 - val_loss: 0.7258 - val_acc: 0.6407\n",
      "Epoch 30/100\n",
      "2307/2307 [==============================] - 1s 456us/step - loss: 0.5575 - acc: 0.7129 - val_loss: 0.7324 - val_acc: 0.6381\n",
      "Epoch 31/100\n",
      "2307/2307 [==============================] - 1s 375us/step - loss: 0.5610 - acc: 0.7125 - val_loss: 0.7603 - val_acc: 0.6355\n",
      "Epoch 32/100\n",
      "2307/2307 [==============================] - 1s 488us/step - loss: 0.5512 - acc: 0.7204 - val_loss: 0.7850 - val_acc: 0.6174\n",
      "Epoch 33/100\n",
      "2307/2307 [==============================] - 1s 445us/step - loss: 0.5518 - acc: 0.7155 - val_loss: 0.7704 - val_acc: 0.6368\n",
      "Epoch 34/100\n",
      "2307/2307 [==============================] - 1s 332us/step - loss: 0.5532 - acc: 0.7161 - val_loss: 0.7950 - val_acc: 0.6342\n",
      "Epoch 35/100\n",
      "2307/2307 [==============================] - 1s 333us/step - loss: 0.5546 - acc: 0.7148 - val_loss: 0.8007 - val_acc: 0.6316\n",
      "Epoch 36/100\n",
      "2307/2307 [==============================] - 1s 358us/step - loss: 0.5486 - acc: 0.7191 - val_loss: 0.8245 - val_acc: 0.6200\n",
      "Epoch 37/100\n",
      "2307/2307 [==============================] - 1s 322us/step - loss: 0.5489 - acc: 0.7222 - val_loss: 0.7819 - val_acc: 0.6226\n",
      "Epoch 38/100\n",
      "2307/2307 [==============================] - 1s 337us/step - loss: 0.5514 - acc: 0.7195 - val_loss: 0.7699 - val_acc: 0.6394\n",
      "Epoch 39/100\n",
      "2307/2307 [==============================] - 1s 316us/step - loss: 0.5452 - acc: 0.7239 - val_loss: 0.7911 - val_acc: 0.6252\n",
      "Epoch 40/100\n",
      "2307/2307 [==============================] - 1s 297us/step - loss: 0.5421 - acc: 0.7258 - val_loss: 0.8166 - val_acc: 0.6239\n",
      "Epoch 41/100\n",
      "2307/2307 [==============================] - 1s 350us/step - loss: 0.5413 - acc: 0.7266 - val_loss: 0.8151 - val_acc: 0.6291\n",
      "Epoch 42/100\n",
      "2307/2307 [==============================] - 1s 341us/step - loss: 0.5422 - acc: 0.7252 - val_loss: 0.8093 - val_acc: 0.6342\n",
      "Epoch 43/100\n",
      "2307/2307 [==============================] - 1s 347us/step - loss: 0.5414 - acc: 0.7240 - val_loss: 0.8124 - val_acc: 0.6342\n",
      "Epoch 44/100\n",
      "2307/2307 [==============================] - 1s 339us/step - loss: 0.5379 - acc: 0.7291 - val_loss: 0.8396 - val_acc: 0.6291\n",
      "Epoch 45/100\n",
      "2307/2307 [==============================] - 1s 340us/step - loss: 0.5391 - acc: 0.7275 - val_loss: 0.8104 - val_acc: 0.6226\n",
      "Epoch 46/100\n",
      "2307/2307 [==============================] - 1s 325us/step - loss: 0.5361 - acc: 0.7271 - val_loss: 0.8223 - val_acc: 0.6291\n",
      "Epoch 47/100\n",
      "2307/2307 [==============================] - 1s 387us/step - loss: 0.5337 - acc: 0.7263 - val_loss: 0.8402 - val_acc: 0.6265\n",
      "Epoch 48/100\n",
      "2307/2307 [==============================] - 1s 302us/step - loss: 0.5337 - acc: 0.7258 - val_loss: 0.8481 - val_acc: 0.6316\n",
      "Epoch 49/100\n",
      "2307/2307 [==============================] - 1s 295us/step - loss: 0.5371 - acc: 0.7259 - val_loss: 0.8406 - val_acc: 0.6252\n",
      "Epoch 50/100\n",
      "2307/2307 [==============================] - 1s 287us/step - loss: 0.5350 - acc: 0.7301 - val_loss: 0.8410 - val_acc: 0.6355\n",
      "Epoch 51/100\n",
      "2307/2307 [==============================] - 1s 279us/step - loss: 0.5394 - acc: 0.7233 - val_loss: 0.8910 - val_acc: 0.6226\n",
      "Epoch 52/100\n",
      "2307/2307 [==============================] - 1s 293us/step - loss: 0.5424 - acc: 0.7222 - val_loss: 0.8388 - val_acc: 0.6304\n",
      "Epoch 53/100\n",
      "2307/2307 [==============================] - 1s 286us/step - loss: 0.5317 - acc: 0.7304 - val_loss: 0.8396 - val_acc: 0.6278\n",
      "Epoch 54/100\n",
      "2307/2307 [==============================] - 1s 282us/step - loss: 0.5262 - acc: 0.7330 - val_loss: 0.8352 - val_acc: 0.6187\n",
      "Epoch 55/100\n",
      "2307/2307 [==============================] - 1s 283us/step - loss: 0.5281 - acc: 0.7330 - val_loss: 0.8534 - val_acc: 0.6329\n",
      "Epoch 56/100\n",
      "2307/2307 [==============================] - 1s 286us/step - loss: 0.5324 - acc: 0.7310 - val_loss: 0.8718 - val_acc: 0.6316\n",
      "Epoch 57/100\n",
      "2307/2307 [==============================] - 1s 282us/step - loss: 0.5258 - acc: 0.7310 - val_loss: 0.8843 - val_acc: 0.6135\n",
      "Epoch 58/100\n",
      "2307/2307 [==============================] - 1s 289us/step - loss: 0.5243 - acc: 0.7344 - val_loss: 0.8372 - val_acc: 0.6342\n",
      "Epoch 59/100\n",
      "2307/2307 [==============================] - 1s 289us/step - loss: 0.5243 - acc: 0.7362 - val_loss: 0.8392 - val_acc: 0.6433\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2307/2307 [==============================] - 1s 286us/step - loss: 0.5271 - acc: 0.7301 - val_loss: 0.8840 - val_acc: 0.6265\n",
      "Epoch 61/100\n",
      "2307/2307 [==============================] - 1s 286us/step - loss: 0.5279 - acc: 0.7288 - val_loss: 0.8735 - val_acc: 0.6291\n",
      "Epoch 62/100\n",
      "2307/2307 [==============================] - 1s 286us/step - loss: 0.5211 - acc: 0.7344 - val_loss: 0.9241 - val_acc: 0.6174\n",
      "Epoch 63/100\n",
      "2307/2307 [==============================] - 1s 298us/step - loss: 0.5366 - acc: 0.7297 - val_loss: 0.9131 - val_acc: 0.6122\n",
      "Epoch 64/100\n",
      "2307/2307 [==============================] - 1s 300us/step - loss: 0.5291 - acc: 0.7307 - val_loss: 0.8625 - val_acc: 0.6407\n",
      "Epoch 65/100\n",
      "2307/2307 [==============================] - 1s 292us/step - loss: 0.5233 - acc: 0.7344 - val_loss: 0.8101 - val_acc: 0.6433\n",
      "Epoch 66/100\n",
      "2307/2307 [==============================] - 1s 289us/step - loss: 0.5311 - acc: 0.7315 - val_loss: 0.8537 - val_acc: 0.6420\n",
      "Epoch 67/100\n",
      "2307/2307 [==============================] - 1s 297us/step - loss: 0.5277 - acc: 0.7272 - val_loss: 0.8980 - val_acc: 0.6316\n",
      "Epoch 68/100\n",
      "2307/2307 [==============================] - 1s 309us/step - loss: 0.5163 - acc: 0.7362 - val_loss: 0.9022 - val_acc: 0.6304\n",
      "Epoch 69/100\n",
      "2307/2307 [==============================] - 1s 296us/step - loss: 0.5199 - acc: 0.7353 - val_loss: 0.8558 - val_acc: 0.6316\n",
      "Epoch 70/100\n",
      "2307/2307 [==============================] - 1s 377us/step - loss: 0.5214 - acc: 0.7341 - val_loss: 0.8739 - val_acc: 0.6407\n",
      "Epoch 71/100\n",
      "2307/2307 [==============================] - 1s 363us/step - loss: 0.5243 - acc: 0.7308 - val_loss: 0.8779 - val_acc: 0.6407\n",
      "Epoch 72/100\n",
      "2307/2307 [==============================] - 1s 403us/step - loss: 0.5184 - acc: 0.7372 - val_loss: 0.9576 - val_acc: 0.6265\n",
      "Epoch 73/100\n",
      "2307/2307 [==============================] - 1s 427us/step - loss: 0.5287 - acc: 0.7284 - val_loss: 0.9052 - val_acc: 0.6265\n",
      "Epoch 74/100\n",
      "2307/2307 [==============================] - 1s 341us/step - loss: 0.5236 - acc: 0.7330 - val_loss: 0.9265 - val_acc: 0.6407\n",
      "Epoch 75/100\n",
      "2307/2307 [==============================] - 1s 312us/step - loss: 0.5193 - acc: 0.7344 - val_loss: 0.9436 - val_acc: 0.6187\n",
      "Epoch 76/100\n",
      "2307/2307 [==============================] - 1s 322us/step - loss: 0.5203 - acc: 0.7372 - val_loss: 0.9405 - val_acc: 0.6213\n",
      "Epoch 77/100\n",
      "2307/2307 [==============================] - 1s 337us/step - loss: 0.5114 - acc: 0.7386 - val_loss: 0.9480 - val_acc: 0.6239\n",
      "Epoch 78/100\n",
      "2307/2307 [==============================] - 1s 351us/step - loss: 0.5118 - acc: 0.7382 - val_loss: 1.0001 - val_acc: 0.6355\n",
      "Epoch 79/100\n",
      "2307/2307 [==============================] - 1s 453us/step - loss: 0.5177 - acc: 0.7395 - val_loss: 1.0212 - val_acc: 0.6200\n",
      "Epoch 80/100\n",
      "2307/2307 [==============================] - 1s 425us/step - loss: 0.5184 - acc: 0.7383 - val_loss: 0.9970 - val_acc: 0.6342\n",
      "Epoch 81/100\n",
      "2307/2307 [==============================] - 1s 347us/step - loss: 0.5167 - acc: 0.7393 - val_loss: 1.0161 - val_acc: 0.6109\n",
      "Epoch 82/100\n",
      "2307/2307 [==============================] - 1s 314us/step - loss: 0.5114 - acc: 0.7378 - val_loss: 0.9444 - val_acc: 0.6200\n",
      "Epoch 83/100\n",
      "2307/2307 [==============================] - 1s 367us/step - loss: 0.5179 - acc: 0.7391 - val_loss: 1.0394 - val_acc: 0.6226\n",
      "Epoch 84/100\n",
      "2307/2307 [==============================] - 1s 310us/step - loss: 0.5218 - acc: 0.7375 - val_loss: 0.8786 - val_acc: 0.6252\n",
      "Epoch 85/100\n",
      "2307/2307 [==============================] - 1s 296us/step - loss: 0.5260 - acc: 0.7298 - val_loss: 0.9609 - val_acc: 0.6174\n",
      "Epoch 86/100\n",
      "2307/2307 [==============================] - 1s 284us/step - loss: 0.5186 - acc: 0.7383 - val_loss: 0.9247 - val_acc: 0.6355\n",
      "Epoch 87/100\n",
      "2307/2307 [==============================] - 1s 286us/step - loss: 0.5178 - acc: 0.7369 - val_loss: 0.9779 - val_acc: 0.6200\n",
      "Epoch 88/100\n",
      "2307/2307 [==============================] - 1s 289us/step - loss: 0.5143 - acc: 0.7382 - val_loss: 1.0110 - val_acc: 0.6291\n",
      "Epoch 89/100\n",
      "2307/2307 [==============================] - 1s 302us/step - loss: 0.5130 - acc: 0.7405 - val_loss: 1.0002 - val_acc: 0.6135\n",
      "Epoch 90/100\n",
      "2307/2307 [==============================] - 1s 285us/step - loss: 0.5042 - acc: 0.7419 - val_loss: 1.0519 - val_acc: 0.6252\n",
      "Epoch 91/100\n",
      "2307/2307 [==============================] - 1s 288us/step - loss: 0.5133 - acc: 0.7353 - val_loss: 1.0646 - val_acc: 0.6161\n",
      "Epoch 92/100\n",
      "2307/2307 [==============================] - 1s 321us/step - loss: 0.5216 - acc: 0.7330 - val_loss: 1.0269 - val_acc: 0.6265\n",
      "Epoch 93/100\n",
      "2307/2307 [==============================] - 1s 290us/step - loss: 0.5100 - acc: 0.7375 - val_loss: 0.9361 - val_acc: 0.6304\n",
      "Epoch 94/100\n",
      "2307/2307 [==============================] - 1s 286us/step - loss: 0.5081 - acc: 0.7379 - val_loss: 0.9983 - val_acc: 0.6174\n",
      "Epoch 95/100\n",
      "2307/2307 [==============================] - 1s 280us/step - loss: 0.5136 - acc: 0.7365 - val_loss: 1.0207 - val_acc: 0.6252\n",
      "Epoch 96/100\n",
      "2307/2307 [==============================] - 1s 282us/step - loss: 0.5202 - acc: 0.7360 - val_loss: 1.0041 - val_acc: 0.6368\n",
      "Epoch 97/100\n",
      "2307/2307 [==============================] - 1s 288us/step - loss: 0.5082 - acc: 0.7396 - val_loss: 1.0213 - val_acc: 0.6122\n",
      "Epoch 98/100\n",
      "2307/2307 [==============================] - 1s 296us/step - loss: 0.5036 - acc: 0.7419 - val_loss: 1.0414 - val_acc: 0.6135\n",
      "Epoch 99/100\n",
      "2307/2307 [==============================] - 1s 285us/step - loss: 0.5005 - acc: 0.7438 - val_loss: 1.0153 - val_acc: 0.6278\n",
      "Epoch 100/100\n",
      "2307/2307 [==============================] - 1s 307us/step - loss: 0.4986 - acc: 0.7451 - val_loss: 1.0525 - val_acc: 0.6291\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Dense(units = 15, kernel_initializer='uniform', activation = 'relu'))\n",
    "classifier.add(Dense(units = 15, kernel_initializer='uniform', activation = 'relu'))\n",
    "classifier.add(Dense(units = 3, kernel_initializer='uniform', activation = 'softmax'))\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "ann = classifier.fit(X_train_tweet, y_train_tweet, batch_size = 10, epochs = 100, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.99\n",
      "acc: 0.62\n"
     ]
    }
   ],
   "source": [
    "score_tweet, acc_tweet = classifier.evaluate(X_test_tweet, y_test_tweet, verbose = 2, batch_size = 32)\n",
    "print(\"score: %.2f\" % (score_tweet))\n",
    "print(\"acc: %.2f\" % (acc_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network for tweets and pca has a 62% accuracy rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search and tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def tweet_model(x_train, y_trainz, x_val, y_val, params):\n",
    "    \n",
    "#     model = Sequential()                            \n",
    "#     model.add(Dense(params['first_neuron'],\n",
    "#                     input_dim=x_train.shape[1],\n",
    "#                     activation='relu'))\n",
    "    \n",
    "#     model.add(Dropout(params['dropout']))\n",
    "#     model.add(Dense(y_trainz.shape[1],\n",
    "#                     activation=params['last_activation']))\n",
    "\n",
    "#     model.compile(optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])),\n",
    "#                   loss=params['loss'],\n",
    "#                   metrics=['acc'])\n",
    "\n",
    "#     out = model.fit(x_train, y_trainz,\n",
    "#                     batch_size=params['batch_size'],\n",
    "#                     epochs=params['epochs'],\n",
    "#                     verbose=0,\n",
    "#                     validation_split = 0.1,\n",
    "#                     callbacks=early_stopper(params['epochs'], mode='strict'))\n",
    "# #     model.fit(x_train, y_train,\n",
    "# #                 batch_size=params['batch_size'],\n",
    "# #                 epochs=params['epochs'],\n",
    "# #                 verbose=0,\n",
    "# #                 validation_data=[x_val, y_val],\n",
    "# #                 callbacks=early_stopper(params['epochs'], mode='strict'))\n",
    "\n",
    "# #     out = model.history\n",
    "#     return out, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we have to make sure to input data and params into the function\n",
    "def tweet_model(x_train, y_train, x_val, y_val, params):\n",
    "    # next we can build the model exactly like we would normally do it\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=x_train.shape[1],\n",
    "                    activation=params['activation'],\n",
    "                    kernel_initializer='normal'))\n",
    "    \n",
    "    model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    # if we want to also test for number of layers and shapes, that's possible\n",
    "#     hidden_layers(model, params, 1)\n",
    "   \n",
    "    # then we finish again with completely standard Keras way\n",
    "    model.add(Dense(3, activation=params['last_activation'],\n",
    "                    kernel_initializer='normal'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  # here we add a regulizer normalization function from Talos\n",
    "                  optimizer=params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer'])),\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0)\n",
    "    \n",
    "    # finally we have to make sure that history object and model are returned\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = Sequential()\n",
    "# classifier.add(Dense(units = 15, kernel_initializer='uniform', activation = 'relu'))\n",
    "# classifier.add(Dense(units = 15, kernel_initializer='uniform', activation = 'relu'))\n",
    "# classifier.add(Dense(units = 3, kernel_initializer='uniform', activation = 'softmax'))\n",
    "# classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# ann = classifier.fit(X_train_tweet, y_train_tweet, batch_size = 10, epochs = 100, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, Nadam\n",
    "from keras.activations import softmax\n",
    "from keras.losses import categorical_crossentropy, logcosh\n",
    "\n",
    "# p = {'lr': (0.1, 10, 10),\n",
    "#      'first_neuron':[4, 8, 16, 32, 64, 128],\n",
    "#      'batch_size': [2, 3, 4],\n",
    "#      'epochs': [200],\n",
    "#      'dropout': (0, 0.40, 10),\n",
    "#      'optimizer': [Adam, Nadam],\n",
    "#      'loss': [categorical_crossentropy, logcosh],\n",
    "#      'last_activation': [softmax],\n",
    "#      'weight_regulizer':[None]}\n",
    "\n",
    "p = {'lr': (0.001, 0.01, .1),\n",
    "     'first_neuron':[4, 8, 16, 32, 64],\n",
    "     'hidden_layers':[0, 1, 2],\n",
    "     'batch_size': (2, 20, 40),\n",
    "     'epochs': [100],\n",
    "     'dropout': (0, 0.5, 0.9),\n",
    "     'weight_regulizer':[None],\n",
    "     'emb_output_dims': [None],\n",
    "#      'shape':['brick','long_funnel'],\n",
    "     'optimizer': [Adam],\n",
    "#      'losses': [logcosh, binary_crossentropy],\n",
    "     'activation':['relu', 'tanh'],\n",
    "     'last_activation': [softmax]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|â–ˆâ–ˆ        | 1/5 [00:29<01:58, 29.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:35<02:01, 40.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:24<01:26, 43.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [05:16<01:21, 81.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [06:36<00:00, 81.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n"
     ]
    }
   ],
   "source": [
    "h = ta.Scan(X_try, y, params=p,\n",
    "            model=tweet_model,\n",
    "            dataset_name='tweet',\n",
    "            experiment_no='1',\n",
    "           grid_downsample=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>weight_regulizer</th>\n",
       "      <th>emb_output_dims</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>lr</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>activation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>0.3504235404267462</td>\n",
       "      <td>5.2842090716332235</td>\n",
       "      <td>0.3544698557400158</td>\n",
       "      <td>5.39117843519873</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;function softmax at 0x1a29e58578&gt;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>0.38074008027623923</td>\n",
       "      <td>1.093950097761779</td>\n",
       "      <td>0.3804573804573805</td>\n",
       "      <td>1.0951174021758556</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;function softmax at 0x1a29e58578&gt;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>tanh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>0.3361569416879066</td>\n",
       "      <td>4.87907732031306</td>\n",
       "      <td>0.35135136076913304</td>\n",
       "      <td>4.808208279574983</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;function softmax at 0x1a29e58578&gt;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>16</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>0.3562193584798225</td>\n",
       "      <td>3.7328899878010735</td>\n",
       "      <td>0.37318088306334807</td>\n",
       "      <td>3.4896931600383736</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;function softmax at 0x1a29e58578&gt;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100</td>\n",
       "      <td>0.34908605572558704</td>\n",
       "      <td>2.814511199681503</td>\n",
       "      <td>0.3118503215623992</td>\n",
       "      <td>3.1697761084334517</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;function softmax at 0x1a29e58578&gt;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  round_epochs                  acc                loss              val_acc  \\\n",
       "1          100   0.3504235404267462  5.2842090716332235   0.3544698557400158   \n",
       "2          100  0.38074008027623923   1.093950097761779   0.3804573804573805   \n",
       "3          100   0.3361569416879066    4.87907732031306  0.35135136076913304   \n",
       "4          100   0.3562193584798225  3.7328899878010735  0.37318088306334807   \n",
       "5          100  0.34908605572558704   2.814511199681503   0.3118503215623992   \n",
       "\n",
       "             val_loss optimizer                             dropout  \\\n",
       "1    5.39117843519873         1  <function softmax at 0x1a29e58578>   \n",
       "2  1.0951174021758556         0  <function softmax at 0x1a29e58578>   \n",
       "3   4.808208279574983         1  <function softmax at 0x1a29e58578>   \n",
       "4  3.4896931600383736         2  <function softmax at 0x1a29e58578>   \n",
       "5  3.1697761084334517         2  <function softmax at 0x1a29e58578>   \n",
       "\n",
       "  batch_size epochs hidden_layers weight_regulizer emb_output_dims  \\\n",
       "1        0.0     19          None              100           0.001   \n",
       "2        0.0      8          None              100           0.001   \n",
       "3        0.0     11          None              100           0.001   \n",
       "4        0.0      3          None              100           0.001   \n",
       "5        0.0      7          None              100           0.001   \n",
       "\n",
       "  last_activation                               lr first_neuron activation  \n",
       "1               8  <class 'keras.optimizers.Adam'>         None       relu  \n",
       "2              32  <class 'keras.optimizers.Adam'>         None       tanh  \n",
       "3              16  <class 'keras.optimizers.Adam'>         None       relu  \n",
       "4               8  <class 'keras.optimizers.Adam'>         None       relu  \n",
       "5               4  <class 'keras.optimizers.Adam'>         None       relu  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accessing the results data frame\n",
    "h.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>acc_epoch</th>\n",
       "      <th>loss_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92</td>\n",
       "      <td>99</td>\n",
       "      <td>95</td>\n",
       "      <td>99</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>7.867708e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>89</td>\n",
       "      <td>99</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>3.071346e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92</td>\n",
       "      <td>99</td>\n",
       "      <td>38</td>\n",
       "      <td>99</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>3.292039e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92</td>\n",
       "      <td>99</td>\n",
       "      <td>95</td>\n",
       "      <td>99</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>4.663215e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>72</td>\n",
       "      <td>99</td>\n",
       "      <td>80</td>\n",
       "      <td>99</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>6.664110e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  acc loss val_acc val_loss  acc_epoch    loss_epoch\n",
       "1  92   99      95       99   0.000059  7.867708e-06\n",
       "2  99   99      89       99   0.000015  3.071346e-07\n",
       "3  92   99      38       99   0.000047  3.292039e-05\n",
       "4  92   99      95       99   0.000100  4.663215e-05\n",
       "5  72   99      80       99   0.000032  6.664110e-05"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accessing epoch entropy values for each round\n",
    "h.peak_epochs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "complete_time            01/23/19/15:28\n",
       "experiment_name                 tweet_1\n",
       "grid_downsample                    0.01\n",
       "random_method          uniform_mersenne\n",
       "reduce_loss                       False\n",
       "reduction_interval                   50\n",
       "reduction_method                   None\n",
       "reduction_metric                val_acc\n",
       "reduction_threshold                 0.2\n",
       "reduction_window                     20\n",
       "x_shape                      (3205, 70)\n",
       "y_shape                       (3205, 3)\n",
       "dtype: object"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# access the summary details\n",
    "h.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use Scan object as input\n",
    "r = ta.Reporting(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0', '32', '<function softmax at 0x1a29e58578>', '0.0',\n",
       "        \"<class 'keras.optimizers.Adam'>\", '8', 'tanh', 'None', 'None',\n",
       "        '100', '0.001', 0],\n",
       "       ['2', '8', '<function softmax at 0x1a29e58578>', '0.0',\n",
       "        \"<class 'keras.optimizers.Adam'>\", '3', 'relu', 'None', 'None',\n",
       "        '100', '0.001', 1],\n",
       "       ['1', '8', '<function softmax at 0x1a29e58578>', '0.0',\n",
       "        \"<class 'keras.optimizers.Adam'>\", '19', 'relu', 'None', 'None',\n",
       "        '100', '0.001', 2],\n",
       "       ['1', '16', '<function softmax at 0x1a29e58578>', '0.0',\n",
       "        \"<class 'keras.optimizers.Adam'>\", '11', 'relu', 'None', 'None',\n",
       "        '100', '0.001', 3],\n",
       "       ['2', '4', '<function softmax at 0x1a29e58578>', '0.0',\n",
       "        \"<class 'keras.optimizers.Adam'>\", '7', 'relu', 'None', 'None',\n",
       "        '100', '0.001', 4]], dtype=object)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.best_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.38074008027623923'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.high('acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
