{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and GLoVe modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 from model_data.csv has tweets that have punctuation and are not lemmatized.\n",
    "# df1 = pd.read_csv('model_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('CLEAN_EDIT_model_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['Unnamed: 0'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_x</th>\n",
       "      <th>retweet_from</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>encoded_sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abort</th>\n",
       "      <th>...</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yr</th>\n",
       "      <th>yup</th>\n",
       "      <th>zero</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stay</td>\n",
       "      <td>137</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>assuming acceleration of to but in a comfortab...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 3858 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  signal_x  retweet_from  tweet_length  encoded_sentiment  polarity  \\\n",
       "0     stay           137           105                  1  0.366667   \n",
       "\n",
       "                                               tweet  ability  able  aboard  \\\n",
       "0  assuming acceleration of to but in a comfortab...      0.0   0.0     0.0   \n",
       "\n",
       "   abort ...   you  your  yours  yourself   yr  yup  zero  zip  zone  zoo  \n",
       "0    0.0 ...   0.0   0.0    0.0       0.0  0.0  0.0   0.0  0.0   0.0  0.0  \n",
       "\n",
       "[1 rows x 3858 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set the tweet column to a string to account of data type errors\n",
    "df.tweet = df.tweet.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the clean dataframe\n",
    "# df.to_csv(r'/Users/sproul/Desktop/ds-projects/project_mod4_AAR/maybe_final_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the class balance of our target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stay    882\n",
       "up      679\n",
       "down    641\n",
       "Name: signal_x, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"signal_x\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a histogram to show the distribution of target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stay    0.400545\n",
       "up      0.308356\n",
       "down    0.291099\n",
       "Name: signal_x, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"signal_x\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set target to the daily move in stock price (or signal_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.signal_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    stay\n",
       "1    stay\n",
       "2    stay\n",
       "3    stay\n",
       "4    stay\n",
       "Name: signal_x, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the target to an array of dummies for our three classes (stay, up, down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this y (the target) for all of our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(target).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(y))\n",
    "y[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial ANN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The initial data for our first model is just the text of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['tweet'].map(word_tokenize).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([list(['assuming', 'acceleration', 'of', 'to', 'but', 'in', 'a', 'comfortable', 'direction', 'will', 'feel', 'like', 'a', 'mild', 'to']),\n",
       "       list(['is', 'capable', 'of', 'transporting', 'satellite', 'to', 'orbit', 'crew', 'and', 'cargo', 'to', 'the', 'and', 'mission', 'to', 'the', 'moon', 'an']),\n",
       "       list(['yup']), ...,\n",
       "       list(['these', 'article', 'in', 'space', 'news', 'describe', 'why', 'v', 't', 'and', 't', 'w']),\n",
       "       list(['wa', 'by', 'a', 'saying', 'rocket', 'ha', 'no', 'chance', 'just', 'said', 'the', 'franco', 'german', 'ha', 'no', 'chance', 'so', 'go', 'with']),\n",
       "       list(['just', 'returned', 'from', 'a', 'trip', 'to', 'and', 'where', 'i', 'met', 'with', 'many', 'interesting', 'people', 'i', 'really', 'like'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(data))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the words with keras preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=15000)\n",
    "tokenizer.fit_on_texts(list(df.tweet))\n",
    "list_tokenized_tweets = tokenizer.texts_to_sequences(df.tweet)\n",
    "X_t = sequence.pad_sequences(list_tokenized_tweets) #, maxlen=2709)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a list of arrays of numbers for each tweet\n",
      "<type 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[979, 532, 4, 2, 20, 6, 3, 1784, 1258, 12, 386, 36, 3, 1785, 2],\n",
       " [5, 805, 4, 1259, 184, 2, 135, 416, 8, 457, 2, 1, 8, 151, 2, 1, 285, 46],\n",
       " [387]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('this is a list of arrays of numbers for each tweet')\n",
    "print(type(list_tokenized_tweets))\n",
    "list_tokenized_tweets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(2202, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,  979,  532,    4,    2,   20,    6,    3, 1784, 1258,\n",
       "          12,  386,   36,    3, 1785,    2],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    5,\n",
       "         805,    4, 1259,  184,    2,  135,  416,    8,  457,    2,    1,\n",
       "           8,  151,    2,    1,  285,   46],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,  387]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(X_t))\n",
    "print(X_t.shape)\n",
    "X_t[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tweet, X_test_tweet, y_train_tweet, y_test_tweet = train_test_split(X_t, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 28 # another test is 128\n",
    "input_ = Input(shape=(28,)) # another test is 100\n",
    "x = Embedding(10000, embedding_size)(input_)\n",
    "x = LSTM(25, return_sequences=True)(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# There are 3 different possible classes, so we use 3 neurons in our output layer\n",
    "x = Dense(3, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 28, 28)            280000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 28, 25)            5400      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                1300      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 286,853\n",
      "Trainable params: 286,853\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1584 samples, validate on 177 samples\n",
      "Epoch 1/20\n",
      "1584/1584 [==============================] - 7s 4ms/step - loss: 1.0961 - acc: 0.3731 - val_loss: 1.0950 - val_acc: 0.3729\n",
      "Epoch 2/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 1.0902 - acc: 0.4078 - val_loss: 1.0953 - val_acc: 0.3729\n",
      "Epoch 3/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 1.0870 - acc: 0.4078 - val_loss: 1.0963 - val_acc: 0.3729\n",
      "Epoch 4/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 1.0775 - acc: 0.4097 - val_loss: 1.0930 - val_acc: 0.3729\n",
      "Epoch 5/20\n",
      "1584/1584 [==============================] - 6s 4ms/step - loss: 1.0387 - acc: 0.4558 - val_loss: 1.0985 - val_acc: 0.3559\n",
      "Epoch 6/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 0.9116 - acc: 0.5802 - val_loss: 1.1495 - val_acc: 0.3616\n",
      "Epoch 7/20\n",
      "1584/1584 [==============================] - 6s 4ms/step - loss: 0.7911 - acc: 0.6237 - val_loss: 1.3625 - val_acc: 0.3616\n",
      "Epoch 8/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 0.7167 - acc: 0.6604 - val_loss: 1.5809 - val_acc: 0.3616\n",
      "Epoch 9/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 0.6177 - acc: 0.7330 - val_loss: 2.1421 - val_acc: 0.3559\n",
      "Epoch 10/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 0.5108 - acc: 0.7759 - val_loss: 2.3910 - val_acc: 0.3559\n",
      "Epoch 11/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 0.4429 - acc: 0.8188 - val_loss: 2.4517 - val_acc: 0.3333\n",
      "Epoch 12/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 0.3672 - acc: 0.8497 - val_loss: 3.0013 - val_acc: 0.3616\n",
      "Epoch 13/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 0.3747 - acc: 0.8396 - val_loss: 2.9673 - val_acc: 0.3390\n",
      "Epoch 14/20\n",
      "1584/1584 [==============================] - 6s 4ms/step - loss: 0.3397 - acc: 0.8630 - val_loss: 3.2227 - val_acc: 0.3785\n",
      "Epoch 15/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 0.3268 - acc: 0.8706 - val_loss: 3.6150 - val_acc: 0.3277\n",
      "Epoch 16/20\n",
      "1584/1584 [==============================] - 6s 3ms/step - loss: 0.2657 - acc: 0.9003 - val_loss: 3.9225 - val_acc: 0.3220\n",
      "Epoch 17/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 0.2267 - acc: 0.9034 - val_loss: 4.5696 - val_acc: 0.3672\n",
      "Epoch 18/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 0.2223 - acc: 0.8971 - val_loss: 4.7772 - val_acc: 0.3616\n",
      "Epoch 19/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 0.2373 - acc: 0.9034 - val_loss: 4.2283 - val_acc: 0.3277\n",
      "Epoch 20/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 0.2111 - acc: 0.9066 - val_loss: 4.9575 - val_acc: 0.3729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3372db90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_tweet, y_train_tweet, epochs=20, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 3.87\n",
      "acc: 0.43\n"
     ]
    }
   ],
   "source": [
    "score_tweet, acc_tweet = model.evaluate(X_test_tweet, y_test_tweet, verbose = 2, batch_size = 32)\n",
    "print(\"score: %.2f\" % (score_tweet))\n",
    "print(\"acc: %.2f\" % (acc_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is the base line Neural Network, with an accuracy of 43%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New model experiments with Random Forest, Support Vector Machine, and Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are testing other models, to get a general baseline before optimizing the Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3235"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocabulary = set(word for tweet in data for word in tweet)\n",
    "len(total_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in that text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'glove.6B/glove.6B.100d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-f6fd605a608f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mglove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove.6B/glove.6B.100d.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B/glove.6B.100d.txt'"
     ]
    }
   ],
   "source": [
    "glove = {}\n",
    "with open('glove.6B/glove.6B.100d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note from Mike: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # It can't be used in a sklearn Pipeline. \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf =  Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "              (\"Random Forest\", RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "              ('Logistic Regression', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('Random Forest', rf),\n",
    "          (\"Support Vector Machine\", svc),\n",
    "          (\"Logistic Regression\", lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [(name, cross_val_score(model, data, target, cv=2).mean()) for name, model, in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.3678468899521531),\n",
       " ('Support Vector Machine', 0.40009074410163337),\n",
       " ('Logistic Regression', 0.36239894406863554)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_x</th>\n",
       "      <th>retweet_from</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>encoded_sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abort</th>\n",
       "      <th>...</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yr</th>\n",
       "      <th>yup</th>\n",
       "      <th>zero</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stay</td>\n",
       "      <td>137</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>assuming acceleration of to but in a comfortab...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stay</td>\n",
       "      <td>64</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>is capable of transporting satellite to orbit ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stay</td>\n",
       "      <td>137</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>yup</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stay</td>\n",
       "      <td>137</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>part</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stay</td>\n",
       "      <td>137</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>fly to most place on earth in under min and an...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3858 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  signal_x  retweet_from  tweet_length  encoded_sentiment  polarity  \\\n",
       "0     stay           137           105                  1  0.366667   \n",
       "1     stay            64           113                  1  0.200000   \n",
       "2     stay           137             6                  0  0.000000   \n",
       "3     stay           137             7                  0  0.000000   \n",
       "4     stay           137            96                  1  0.650000   \n",
       "\n",
       "                                               tweet  ability  able  aboard  \\\n",
       "0  assuming acceleration of to but in a comfortab...      0.0   0.0     0.0   \n",
       "1  is capable of transporting satellite to orbit ...      0.0   0.0     0.0   \n",
       "2                                                yup      0.0   0.0     0.0   \n",
       "3                                               part      0.0   0.0     0.0   \n",
       "4  fly to most place on earth in under min and an...      0.0   0.0     0.0   \n",
       "\n",
       "   abort ...   you  your  yours  yourself   yr  yup  zero  zip  zone  zoo  \n",
       "0    0.0 ...   0.0   0.0    0.0       0.0  0.0  0.0   0.0  0.0   0.0  0.0  \n",
       "1    0.0 ...   0.0   0.0    0.0       0.0  0.0  0.0   0.0  0.0   0.0  0.0  \n",
       "2    0.0 ...   0.0   0.0    0.0       0.0  0.0  1.0   0.0  0.0   0.0  0.0  \n",
       "3    0.0 ...   0.0   0.0    0.0       0.0  0.0  0.0   0.0  0.0   0.0  0.0  \n",
       "4    0.0 ...   0.0   0.0    0.0       0.0  0.0  0.0   0.0  0.0   0.0  0.0  \n",
       "\n",
       "[5 rows x 3858 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,  979,  532,    4,    2,   20,    6,    3, 1784, 1258,\n",
       "          12,  386,   36,    3, 1785,    2],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    5,\n",
       "         805,    4, 1259,  184,    2,  135,  416,    8,  457,    2,    1,\n",
       "           8,  151,    2,    1,  285,   46],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,  387]], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the tweet data going into the model\n",
    "X_t[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ann, X_test_ann, y_train_ann, y_test_ann = train_test_split(X_t, y, test_size = 0.2, random_state = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1584 samples, validate on 177 samples\n",
      "Epoch 1/100\n",
      "1584/1584 [==============================] - 1s 873us/step - loss: 0.6390 - acc: 0.6595 - val_loss: 0.6349 - val_acc: 0.6667\n",
      "Epoch 2/100\n",
      "1584/1584 [==============================] - 1s 332us/step - loss: 0.6291 - acc: 0.6660 - val_loss: 0.6354 - val_acc: 0.6648\n",
      "Epoch 3/100\n",
      "1584/1584 [==============================] - 1s 324us/step - loss: 0.6230 - acc: 0.6726 - val_loss: 0.6404 - val_acc: 0.6591\n",
      "Epoch 4/100\n",
      "1584/1584 [==============================] - 1s 332us/step - loss: 0.6185 - acc: 0.6700 - val_loss: 0.6418 - val_acc: 0.6573\n",
      "Epoch 5/100\n",
      "1584/1584 [==============================] - 1s 331us/step - loss: 0.6165 - acc: 0.6753 - val_loss: 0.6550 - val_acc: 0.6460\n",
      "Epoch 6/100\n",
      "1584/1584 [==============================] - 1s 329us/step - loss: 0.6094 - acc: 0.6810 - val_loss: 0.6531 - val_acc: 0.6535\n",
      "Epoch 7/100\n",
      "1584/1584 [==============================] - 1s 402us/step - loss: 0.6061 - acc: 0.6822 - val_loss: 0.6489 - val_acc: 0.6610\n",
      "Epoch 8/100\n",
      "1584/1584 [==============================] - 1s 460us/step - loss: 0.6046 - acc: 0.6856 - val_loss: 0.6514 - val_acc: 0.6629\n",
      "Epoch 9/100\n",
      "1584/1584 [==============================] - 1s 398us/step - loss: 0.6008 - acc: 0.6873 - val_loss: 0.6638 - val_acc: 0.6573\n",
      "Epoch 10/100\n",
      "1584/1584 [==============================] - 1s 440us/step - loss: 0.5938 - acc: 0.6873 - val_loss: 0.6754 - val_acc: 0.6573\n",
      "Epoch 11/100\n",
      "1584/1584 [==============================] - 1s 403us/step - loss: 0.5883 - acc: 0.6959 - val_loss: 0.7253 - val_acc: 0.6460\n",
      "Epoch 12/100\n",
      "1584/1584 [==============================] - 1s 450us/step - loss: 0.5849 - acc: 0.6955 - val_loss: 0.7031 - val_acc: 0.6403\n",
      "Epoch 13/100\n",
      "1584/1584 [==============================] - 1s 444us/step - loss: 0.5758 - acc: 0.7016 - val_loss: 0.7542 - val_acc: 0.6535\n",
      "Epoch 14/100\n",
      "1584/1584 [==============================] - 1s 382us/step - loss: 0.5732 - acc: 0.7035 - val_loss: 0.7861 - val_acc: 0.6478\n",
      "Epoch 15/100\n",
      "1584/1584 [==============================] - 1s 448us/step - loss: 0.5716 - acc: 0.7066 - val_loss: 0.8051 - val_acc: 0.6403\n",
      "Epoch 16/100\n",
      "1584/1584 [==============================] - 1s 369us/step - loss: 0.5644 - acc: 0.7062 - val_loss: 0.8035 - val_acc: 0.6497\n",
      "Epoch 17/100\n",
      "1584/1584 [==============================] - 1s 395us/step - loss: 0.5629 - acc: 0.7071 - val_loss: 0.8157 - val_acc: 0.6516\n",
      "Epoch 18/100\n",
      "1584/1584 [==============================] - 1s 387us/step - loss: 0.5594 - acc: 0.7096 - val_loss: 0.8233 - val_acc: 0.6422\n",
      "Epoch 19/100\n",
      "1584/1584 [==============================] - 1s 425us/step - loss: 0.5594 - acc: 0.7104 - val_loss: 0.8410 - val_acc: 0.6403\n",
      "Epoch 20/100\n",
      "1584/1584 [==============================] - 1s 403us/step - loss: 0.5527 - acc: 0.7132 - val_loss: 0.8724 - val_acc: 0.6422\n",
      "Epoch 21/100\n",
      "1584/1584 [==============================] - 1s 398us/step - loss: 0.5467 - acc: 0.7178 - val_loss: 0.8935 - val_acc: 0.6554\n",
      "Epoch 22/100\n",
      "1584/1584 [==============================] - 1s 490us/step - loss: 0.5508 - acc: 0.7132 - val_loss: 0.8523 - val_acc: 0.6478\n",
      "Epoch 23/100\n",
      "1584/1584 [==============================] - 1s 490us/step - loss: 0.5426 - acc: 0.7151 - val_loss: 0.8870 - val_acc: 0.6422\n",
      "Epoch 24/100\n",
      "1584/1584 [==============================] - 1s 394us/step - loss: 0.5389 - acc: 0.7216 - val_loss: 0.9521 - val_acc: 0.6328\n",
      "Epoch 25/100\n",
      "1584/1584 [==============================] - 1s 352us/step - loss: 0.5440 - acc: 0.7182 - val_loss: 0.9160 - val_acc: 0.6290\n",
      "Epoch 26/100\n",
      "1584/1584 [==============================] - 1s 342us/step - loss: 0.5390 - acc: 0.7197 - val_loss: 0.9726 - val_acc: 0.6460\n",
      "Epoch 27/100\n",
      "1584/1584 [==============================] - 1s 355us/step - loss: 0.5351 - acc: 0.7239 - val_loss: 0.9612 - val_acc: 0.6347\n",
      "Epoch 28/100\n",
      "1584/1584 [==============================] - 1s 359us/step - loss: 0.5434 - acc: 0.7212 - val_loss: 0.9415 - val_acc: 0.6516\n",
      "Epoch 29/100\n",
      "1584/1584 [==============================] - 1s 350us/step - loss: 0.5279 - acc: 0.7231 - val_loss: 0.9003 - val_acc: 0.6422\n",
      "Epoch 30/100\n",
      "1584/1584 [==============================] - 1s 429us/step - loss: 0.5308 - acc: 0.7239 - val_loss: 0.9501 - val_acc: 0.6460\n",
      "Epoch 31/100\n",
      "1584/1584 [==============================] - 1s 388us/step - loss: 0.5308 - acc: 0.7237 - val_loss: 0.9420 - val_acc: 0.6460\n",
      "Epoch 32/100\n",
      "1584/1584 [==============================] - 1s 401us/step - loss: 0.5236 - acc: 0.7279 - val_loss: 1.0500 - val_acc: 0.6384\n",
      "Epoch 33/100\n",
      "1584/1584 [==============================] - 1s 391us/step - loss: 0.5245 - acc: 0.7279 - val_loss: 1.0408 - val_acc: 0.6403\n",
      "Epoch 34/100\n",
      "1584/1584 [==============================] - 1s 356us/step - loss: 0.5178 - acc: 0.7279 - val_loss: 1.1175 - val_acc: 0.6309\n",
      "Epoch 35/100\n",
      "1584/1584 [==============================] - 1s 364us/step - loss: 0.5285 - acc: 0.7292 - val_loss: 1.1207 - val_acc: 0.6422\n",
      "Epoch 36/100\n",
      "1584/1584 [==============================] - 1s 334us/step - loss: 0.5302 - acc: 0.7269 - val_loss: 1.0971 - val_acc: 0.6347\n",
      "Epoch 37/100\n",
      "1584/1584 [==============================] - 1s 332us/step - loss: 0.5266 - acc: 0.7294 - val_loss: 1.0720 - val_acc: 0.6347\n",
      "Epoch 38/100\n",
      "1584/1584 [==============================] - 1s 320us/step - loss: 0.5091 - acc: 0.7330 - val_loss: 1.1965 - val_acc: 0.6328\n",
      "Epoch 39/100\n",
      "1584/1584 [==============================] - 1s 331us/step - loss: 0.5208 - acc: 0.7285 - val_loss: 1.1436 - val_acc: 0.6290\n",
      "Epoch 40/100\n",
      "1584/1584 [==============================] - 1s 324us/step - loss: 0.5185 - acc: 0.7296 - val_loss: 1.0644 - val_acc: 0.6403\n",
      "Epoch 41/100\n",
      "1584/1584 [==============================] - 1s 342us/step - loss: 0.5071 - acc: 0.7290 - val_loss: 1.1113 - val_acc: 0.6328\n",
      "Epoch 42/100\n",
      "1584/1584 [==============================] - 1s 344us/step - loss: 0.5079 - acc: 0.7338 - val_loss: 1.1409 - val_acc: 0.6347\n",
      "Epoch 43/100\n",
      "1584/1584 [==============================] - 1s 330us/step - loss: 0.5010 - acc: 0.7410 - val_loss: 1.1593 - val_acc: 0.6347\n",
      "Epoch 44/100\n",
      "1584/1584 [==============================] - 1s 329us/step - loss: 0.5014 - acc: 0.7407 - val_loss: 1.1827 - val_acc: 0.6271\n",
      "Epoch 45/100\n",
      "1584/1584 [==============================] - 1s 331us/step - loss: 0.4988 - acc: 0.7376 - val_loss: 1.2356 - val_acc: 0.6309\n",
      "Epoch 46/100\n",
      "1584/1584 [==============================] - 1s 365us/step - loss: 0.4974 - acc: 0.7397 - val_loss: 1.1564 - val_acc: 0.6365\n",
      "Epoch 47/100\n",
      "1584/1584 [==============================] - 1s 324us/step - loss: 0.4996 - acc: 0.7376 - val_loss: 1.1852 - val_acc: 0.6328\n",
      "Epoch 48/100\n",
      "1584/1584 [==============================] - 1s 344us/step - loss: 0.5073 - acc: 0.7315 - val_loss: 1.1630 - val_acc: 0.6328\n",
      "Epoch 49/100\n",
      "1584/1584 [==============================] - 1s 325us/step - loss: 0.5138 - acc: 0.7332 - val_loss: 1.1198 - val_acc: 0.6384\n",
      "Epoch 50/100\n",
      "1584/1584 [==============================] - 1s 375us/step - loss: 0.5052 - acc: 0.7344 - val_loss: 1.2026 - val_acc: 0.6290\n",
      "Epoch 51/100\n",
      "1584/1584 [==============================] - 1s 335us/step - loss: 0.5028 - acc: 0.7407 - val_loss: 1.1393 - val_acc: 0.6252\n",
      "Epoch 52/100\n",
      "1584/1584 [==============================] - 1s 346us/step - loss: 0.4936 - acc: 0.7426 - val_loss: 1.2026 - val_acc: 0.6347\n",
      "Epoch 53/100\n",
      "1584/1584 [==============================] - 1s 323us/step - loss: 0.4928 - acc: 0.7422 - val_loss: 1.2034 - val_acc: 0.6384\n",
      "Epoch 54/100\n",
      "1584/1584 [==============================] - 1s 346us/step - loss: 0.5046 - acc: 0.7399 - val_loss: 1.2207 - val_acc: 0.6403\n",
      "Epoch 55/100\n",
      "1584/1584 [==============================] - 1s 334us/step - loss: 0.4984 - acc: 0.7401 - val_loss: 1.2284 - val_acc: 0.6403\n",
      "Epoch 56/100\n",
      "1584/1584 [==============================] - 1s 399us/step - loss: 0.4891 - acc: 0.7466 - val_loss: 1.2229 - val_acc: 0.6328\n",
      "Epoch 57/100\n",
      "1584/1584 [==============================] - 1s 398us/step - loss: 0.4917 - acc: 0.7452 - val_loss: 1.2975 - val_acc: 0.6234\n",
      "Epoch 58/100\n",
      "1584/1584 [==============================] - 1s 369us/step - loss: 0.4945 - acc: 0.7388 - val_loss: 1.2953 - val_acc: 0.6403\n",
      "Epoch 59/100\n",
      "1584/1584 [==============================] - 1s 378us/step - loss: 0.4965 - acc: 0.7393 - val_loss: 1.1948 - val_acc: 0.6441\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1584/1584 [==============================] - 1s 358us/step - loss: 0.4896 - acc: 0.7502 - val_loss: 1.2595 - val_acc: 0.6309\n",
      "Epoch 61/100\n",
      "1584/1584 [==============================] - 0s 314us/step - loss: 0.5006 - acc: 0.7401 - val_loss: 1.2133 - val_acc: 0.6441\n",
      "Epoch 62/100\n",
      "1584/1584 [==============================] - 0s 315us/step - loss: 0.4943 - acc: 0.7452 - val_loss: 1.2633 - val_acc: 0.6271\n",
      "Epoch 63/100\n",
      "1584/1584 [==============================] - 1s 324us/step - loss: 0.5165 - acc: 0.7393 - val_loss: 1.1773 - val_acc: 0.6384\n",
      "Epoch 64/100\n",
      "1584/1584 [==============================] - 1s 316us/step - loss: 0.5002 - acc: 0.7397 - val_loss: 1.3251 - val_acc: 0.6234\n",
      "Epoch 65/100\n",
      "1584/1584 [==============================] - 1s 317us/step - loss: 0.4889 - acc: 0.7447 - val_loss: 1.3160 - val_acc: 0.6252\n",
      "Epoch 66/100\n",
      "1584/1584 [==============================] - 0s 315us/step - loss: 0.4888 - acc: 0.7477 - val_loss: 1.2908 - val_acc: 0.6365\n",
      "Epoch 67/100\n",
      "1584/1584 [==============================] - 1s 318us/step - loss: 0.4835 - acc: 0.7519 - val_loss: 1.2783 - val_acc: 0.6271\n",
      "Epoch 68/100\n",
      "1584/1584 [==============================] - 0s 312us/step - loss: 0.4756 - acc: 0.7498 - val_loss: 1.2933 - val_acc: 0.6403\n",
      "Epoch 69/100\n",
      "1584/1584 [==============================] - 1s 333us/step - loss: 0.4748 - acc: 0.7519 - val_loss: 1.2466 - val_acc: 0.6591\n",
      "Epoch 70/100\n",
      "1584/1584 [==============================] - 0s 307us/step - loss: 0.4731 - acc: 0.7563 - val_loss: 1.3394 - val_acc: 0.6290\n",
      "Epoch 71/100\n",
      "1584/1584 [==============================] - 1s 321us/step - loss: 0.4784 - acc: 0.7489 - val_loss: 1.3461 - val_acc: 0.6271\n",
      "Epoch 72/100\n",
      "1584/1584 [==============================] - 1s 317us/step - loss: 0.5191 - acc: 0.7403 - val_loss: 1.3075 - val_acc: 0.6328\n",
      "Epoch 73/100\n",
      "1584/1584 [==============================] - 1s 322us/step - loss: 0.5053 - acc: 0.7458 - val_loss: 1.3443 - val_acc: 0.6403\n",
      "Epoch 74/100\n",
      "1584/1584 [==============================] - 0s 313us/step - loss: 0.4873 - acc: 0.7494 - val_loss: 1.3384 - val_acc: 0.6328\n",
      "Epoch 75/100\n",
      "1584/1584 [==============================] - 1s 318us/step - loss: 0.4872 - acc: 0.7494 - val_loss: 1.3120 - val_acc: 0.6328\n",
      "Epoch 76/100\n",
      "1584/1584 [==============================] - 1s 318us/step - loss: 0.4857 - acc: 0.7548 - val_loss: 1.3632 - val_acc: 0.6215\n",
      "Epoch 77/100\n",
      "1584/1584 [==============================] - 1s 321us/step - loss: 0.4896 - acc: 0.7456 - val_loss: 1.3770 - val_acc: 0.6384\n",
      "Epoch 78/100\n",
      "1584/1584 [==============================] - 1s 325us/step - loss: 0.4774 - acc: 0.7565 - val_loss: 1.4579 - val_acc: 0.6252\n",
      "Epoch 79/100\n",
      "1584/1584 [==============================] - 1s 353us/step - loss: 0.4771 - acc: 0.7561 - val_loss: 1.3793 - val_acc: 0.6365\n",
      "Epoch 80/100\n",
      "1584/1584 [==============================] - 1s 335us/step - loss: 0.4717 - acc: 0.7582 - val_loss: 1.3323 - val_acc: 0.6347\n",
      "Epoch 81/100\n",
      "1584/1584 [==============================] - 1s 318us/step - loss: 0.4696 - acc: 0.7586 - val_loss: 1.3310 - val_acc: 0.6441\n",
      "Epoch 82/100\n",
      "1584/1584 [==============================] - 1s 335us/step - loss: 0.4746 - acc: 0.7527 - val_loss: 1.3836 - val_acc: 0.6271\n",
      "Epoch 83/100\n",
      "1584/1584 [==============================] - 1s 329us/step - loss: 0.4728 - acc: 0.7578 - val_loss: 1.3421 - val_acc: 0.6365\n",
      "Epoch 84/100\n",
      "1584/1584 [==============================] - 1s 325us/step - loss: 0.4669 - acc: 0.7601 - val_loss: 1.4301 - val_acc: 0.6196\n",
      "Epoch 85/100\n",
      "1584/1584 [==============================] - 1s 317us/step - loss: 0.4663 - acc: 0.7590 - val_loss: 1.4165 - val_acc: 0.6234\n",
      "Epoch 86/100\n",
      "1584/1584 [==============================] - 1s 333us/step - loss: 0.4720 - acc: 0.7559 - val_loss: 1.4108 - val_acc: 0.6328\n",
      "Epoch 87/100\n",
      "1584/1584 [==============================] - 0s 316us/step - loss: 0.4645 - acc: 0.7624 - val_loss: 1.4905 - val_acc: 0.6271\n",
      "Epoch 88/100\n",
      "1584/1584 [==============================] - 1s 329us/step - loss: 0.4641 - acc: 0.7643 - val_loss: 1.4721 - val_acc: 0.6365\n",
      "Epoch 89/100\n",
      "1584/1584 [==============================] - 1s 328us/step - loss: 0.4715 - acc: 0.7599 - val_loss: 1.5705 - val_acc: 0.6403\n",
      "Epoch 90/100\n",
      "1584/1584 [==============================] - 1s 406us/step - loss: 0.5165 - acc: 0.7418 - val_loss: 1.3420 - val_acc: 0.6497\n",
      "Epoch 91/100\n",
      "1584/1584 [==============================] - 1s 370us/step - loss: 0.4748 - acc: 0.7574 - val_loss: 1.4445 - val_acc: 0.6347\n",
      "Epoch 92/100\n",
      "1584/1584 [==============================] - 1s 378us/step - loss: 0.4809 - acc: 0.7580 - val_loss: 1.4065 - val_acc: 0.6365\n",
      "Epoch 93/100\n",
      "1584/1584 [==============================] - 1s 361us/step - loss: 0.4689 - acc: 0.7576 - val_loss: 1.5027 - val_acc: 0.6177\n",
      "Epoch 94/100\n",
      "1584/1584 [==============================] - 1s 402us/step - loss: 0.4648 - acc: 0.7645 - val_loss: 1.4955 - val_acc: 0.6177\n",
      "Epoch 95/100\n",
      "1584/1584 [==============================] - 1s 370us/step - loss: 0.4744 - acc: 0.7593 - val_loss: 1.5427 - val_acc: 0.6158\n",
      "Epoch 96/100\n",
      "1584/1584 [==============================] - 1s 323us/step - loss: 0.4700 - acc: 0.7563 - val_loss: 1.5134 - val_acc: 0.6328\n",
      "Epoch 97/100\n",
      "1584/1584 [==============================] - 1s 348us/step - loss: 0.4630 - acc: 0.7593 - val_loss: 1.5214 - val_acc: 0.6328\n",
      "Epoch 98/100\n",
      "1584/1584 [==============================] - 1s 326us/step - loss: 0.4558 - acc: 0.7662 - val_loss: 1.5278 - val_acc: 0.6215\n",
      "Epoch 99/100\n",
      "1584/1584 [==============================] - 1s 335us/step - loss: 0.4565 - acc: 0.7675 - val_loss: 1.6008 - val_acc: 0.6158\n",
      "Epoch 100/100\n",
      "1584/1584 [==============================] - 1s 333us/step - loss: 0.4826 - acc: 0.7593 - val_loss: 1.4369 - val_acc: 0.6252\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Dense(units = 15, kernel_initializer='uniform', activation = 'relu'))\n",
    "classifier.add(Dense(units = 15, kernel_initializer='uniform', activation = 'relu'))\n",
    "classifier.add(Dense(units = 3, kernel_initializer='uniform', activation = 'softmax'))\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# ann = classifier.fit(train_final, label_train_final, batch_size = 10, epochs = 100, validation_data=(val, label_val))\n",
    "ann = classifier.fit(X_train_ann, y_train_ann, batch_size = 10, epochs = 100, validation_split=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1761/1761 [==============================] - 0s 41us/step\n"
     ]
    }
   ],
   "source": [
    "results_train_ann = classifier.evaluate(X_train_ann, y_train_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441/441 [==============================] - 0s 52us/step\n"
     ]
    }
   ],
   "source": [
    "results_test_ann = classifier.evaluate(X_test_ann, y_test_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5555036427079785, 0.7533598247744155]\n",
      "[1.2675651276733328, 0.6182917620049042]\n"
     ]
    }
   ],
   "source": [
    "print(results_train_ann)\n",
    "print(results_test_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for ann model: 1.27\n",
      "acc for ann model: 0.62\n"
     ]
    }
   ],
   "source": [
    "score_ann, acc_ann = classifier.evaluate(X_test_ann, y_test_ann, verbose = 2, batch_size = 32)\n",
    "print(\"score for ann model: %.2f\" % (score_ann))\n",
    "print(\"acc for ann model: %.2f\" % (acc_ann))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network for tweets has a 62% accuracy rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN for non tweet features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the dataframe for not tweet features, which include the tf-idf of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_no_tweet = df.drop(['signal_x', 'tweet'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweet_from</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>encoded_sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abort</th>\n",
       "      <th>aborted</th>\n",
       "      <th>about</th>\n",
       "      <th>...</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yr</th>\n",
       "      <th>yup</th>\n",
       "      <th>zero</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>137</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3856 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   retweet_from  tweet_length  encoded_sentiment  polarity  ability  able  \\\n",
       "0           137           105                  1  0.366667      0.0   0.0   \n",
       "1            64           113                  1  0.200000      0.0   0.0   \n",
       "2           137             6                  0  0.000000      0.0   0.0   \n",
       "3           137             7                  0  0.000000      0.0   0.0   \n",
       "4           137            96                  1  0.650000      0.0   0.0   \n",
       "\n",
       "   aboard  abort  aborted  about ...   you  your  yours  yourself   yr  yup  \\\n",
       "0     0.0    0.0      0.0    0.0 ...   0.0   0.0    0.0       0.0  0.0  0.0   \n",
       "1     0.0    0.0      0.0    0.0 ...   0.0   0.0    0.0       0.0  0.0  0.0   \n",
       "2     0.0    0.0      0.0    0.0 ...   0.0   0.0    0.0       0.0  0.0  1.0   \n",
       "3     0.0    0.0      0.0    0.0 ...   0.0   0.0    0.0       0.0  0.0  0.0   \n",
       "4     0.0    0.0      0.0    0.0 ...   0.0   0.0    0.0       0.0  0.0  0.0   \n",
       "\n",
       "   zero  zip  zone  zoo  \n",
       "0   0.0  0.0   0.0  0.0  \n",
       "1   0.0  0.0   0.0  0.0  \n",
       "2   0.0  0.0   0.0  0.0  \n",
       "3   0.0  0.0   0.0  0.0  \n",
       "4   0.0  0.0   0.0  0.0  \n",
       "\n",
       "[5 rows x 3856 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roberthillery/anaconda3/envs/venv/lib/python2.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# these are the features for the no_tweet ANN model\n",
    "X = df_no_tweet.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[137., 105.,   1., ...,   0.,   0.,   0.],\n",
       "       [ 64., 113.,   1., ...,   0.,   0.,   0.],\n",
       "       [137.,   6.,   0., ...,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# this is check the target for the Neural Network\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1584 samples, validate on 177 samples\n",
      "Epoch 1/100\n",
      "1584/1584 [==============================] - 3s 2ms/step - loss: 0.6325 - acc: 0.6667 - val_loss: 0.6388 - val_acc: 0.6667\n",
      "Epoch 2/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6304 - acc: 0.6665 - val_loss: 0.6374 - val_acc: 0.6667\n",
      "Epoch 3/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6311 - acc: 0.6667 - val_loss: 0.6373 - val_acc: 0.6667\n",
      "Epoch 4/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6300 - acc: 0.6667 - val_loss: 0.6382 - val_acc: 0.6667\n",
      "Epoch 5/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6298 - acc: 0.6667 - val_loss: 0.6380 - val_acc: 0.6667\n",
      "Epoch 6/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6298 - acc: 0.6665 - val_loss: 0.6369 - val_acc: 0.6667\n",
      "Epoch 7/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6289 - acc: 0.6667 - val_loss: 0.6397 - val_acc: 0.6667\n",
      "Epoch 8/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6283 - acc: 0.6667 - val_loss: 0.6395 - val_acc: 0.6667\n",
      "Epoch 9/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6275 - acc: 0.6667 - val_loss: 0.6384 - val_acc: 0.6667\n",
      "Epoch 10/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6248 - acc: 0.6660 - val_loss: 0.6407 - val_acc: 0.6667\n",
      "Epoch 11/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6219 - acc: 0.6723 - val_loss: 0.6383 - val_acc: 0.6667\n",
      "Epoch 12/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6148 - acc: 0.6751 - val_loss: 0.6421 - val_acc: 0.6667\n",
      "Epoch 13/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.5997 - acc: 0.6987 - val_loss: 0.6453 - val_acc: 0.6610\n",
      "Epoch 14/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.5513 - acc: 0.7229 - val_loss: 0.6813 - val_acc: 0.6591\n",
      "Epoch 15/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.4931 - acc: 0.7441 - val_loss: 0.8941 - val_acc: 0.5857\n",
      "Epoch 16/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.4705 - acc: 0.7492 - val_loss: 0.7185 - val_acc: 0.6422\n",
      "Epoch 17/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.4390 - acc: 0.7670 - val_loss: 0.7573 - val_acc: 0.6328\n",
      "Epoch 18/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.4323 - acc: 0.7612 - val_loss: 0.7519 - val_acc: 0.6347\n",
      "Epoch 19/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.4134 - acc: 0.7725 - val_loss: 0.7691 - val_acc: 0.6121\n",
      "Epoch 20/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.3963 - acc: 0.7837 - val_loss: 0.8267 - val_acc: 0.6121\n",
      "Epoch 21/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.3938 - acc: 0.7862 - val_loss: 0.7946 - val_acc: 0.6045\n",
      "Epoch 22/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.3731 - acc: 0.8049 - val_loss: 0.9315 - val_acc: 0.6045\n",
      "Epoch 23/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.3751 - acc: 0.8083 - val_loss: 1.0570 - val_acc: 0.6026\n",
      "Epoch 24/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.3634 - acc: 0.8243 - val_loss: 0.8530 - val_acc: 0.6083\n",
      "Epoch 25/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.3414 - acc: 0.8371 - val_loss: 0.8698 - val_acc: 0.5951\n",
      "Epoch 26/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.3384 - acc: 0.8504 - val_loss: 0.9663 - val_acc: 0.5913\n",
      "Epoch 27/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.3164 - acc: 0.8533 - val_loss: 0.9171 - val_acc: 0.5913\n",
      "Epoch 28/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.3000 - acc: 0.8718 - val_loss: 1.0132 - val_acc: 0.5819\n",
      "Epoch 29/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.2881 - acc: 0.8857 - val_loss: 0.9521 - val_acc: 0.5574\n",
      "Epoch 30/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.2664 - acc: 0.8933 - val_loss: 0.9730 - val_acc: 0.5687\n",
      "Epoch 31/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.2549 - acc: 0.9021 - val_loss: 1.0588 - val_acc: 0.5876\n",
      "Epoch 32/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.2265 - acc: 0.9135 - val_loss: 1.0096 - val_acc: 0.5725\n",
      "Epoch 33/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.2018 - acc: 0.9333 - val_loss: 1.0774 - val_acc: 0.5782\n",
      "Epoch 34/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.2236 - acc: 0.9076 - val_loss: 1.1506 - val_acc: 0.5725\n",
      "Epoch 35/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1841 - acc: 0.9354 - val_loss: 1.1328 - val_acc: 0.5782\n",
      "Epoch 36/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1646 - acc: 0.9402 - val_loss: 1.1229 - val_acc: 0.5687\n",
      "Epoch 37/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1539 - acc: 0.9474 - val_loss: 1.1270 - val_acc: 0.5669\n",
      "Epoch 38/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1617 - acc: 0.9394 - val_loss: 1.2954 - val_acc: 0.5800\n",
      "Epoch 39/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1432 - acc: 0.9413 - val_loss: 1.3728 - val_acc: 0.5819\n",
      "Epoch 40/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1292 - acc: 0.9520 - val_loss: 1.2945 - val_acc: 0.5819\n",
      "Epoch 41/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1150 - acc: 0.9600 - val_loss: 1.2262 - val_acc: 0.5669\n",
      "Epoch 42/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1072 - acc: 0.9632 - val_loss: 1.3610 - val_acc: 0.5556\n",
      "Epoch 43/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1017 - acc: 0.9611 - val_loss: 1.3129 - val_acc: 0.5612\n",
      "Epoch 44/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1053 - acc: 0.9588 - val_loss: 1.3829 - val_acc: 0.5989\n",
      "Epoch 45/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1053 - acc: 0.9577 - val_loss: 1.3147 - val_acc: 0.5706\n",
      "Epoch 46/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0911 - acc: 0.9642 - val_loss: 1.4255 - val_acc: 0.5593\n",
      "Epoch 47/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0891 - acc: 0.9640 - val_loss: 1.4313 - val_acc: 0.5518\n",
      "Epoch 48/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0860 - acc: 0.9670 - val_loss: 1.4019 - val_acc: 0.5706\n",
      "Epoch 49/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1017 - acc: 0.9606 - val_loss: 1.3755 - val_acc: 0.5706\n",
      "Epoch 50/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0844 - acc: 0.9640 - val_loss: 1.6435 - val_acc: 0.5518\n",
      "Epoch 51/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0785 - acc: 0.9680 - val_loss: 1.6079 - val_acc: 0.5895\n",
      "Epoch 52/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0910 - acc: 0.9636 - val_loss: 1.5051 - val_acc: 0.5744\n",
      "Epoch 53/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0806 - acc: 0.9665 - val_loss: 1.5868 - val_acc: 0.5574\n",
      "Epoch 54/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0817 - acc: 0.9665 - val_loss: 1.5193 - val_acc: 0.5556\n",
      "Epoch 55/100\n",
      "1584/1584 [==============================] - 3s 2ms/step - loss: 0.0756 - acc: 0.9668 - val_loss: 1.6158 - val_acc: 0.5650\n",
      "Epoch 56/100\n",
      "1584/1584 [==============================] - 2s 2ms/step - loss: 0.0697 - acc: 0.9712 - val_loss: 1.6281 - val_acc: 0.5443\n",
      "Epoch 57/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0735 - acc: 0.9684 - val_loss: 1.5181 - val_acc: 0.5631\n",
      "Epoch 58/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0721 - acc: 0.9701 - val_loss: 1.7790 - val_acc: 0.5895\n",
      "Epoch 59/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0750 - acc: 0.9691 - val_loss: 2.0147 - val_acc: 0.5913\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0777 - acc: 0.9672 - val_loss: 1.7889 - val_acc: 0.5612\n",
      "Epoch 61/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0653 - acc: 0.9693 - val_loss: 2.0223 - val_acc: 0.5763\n",
      "Epoch 62/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0696 - acc: 0.9718 - val_loss: 1.6906 - val_acc: 0.5669\n",
      "Epoch 63/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0719 - acc: 0.9676 - val_loss: 1.7851 - val_acc: 0.5424\n",
      "Epoch 64/100\n",
      "1584/1584 [==============================] - 2s 968us/step - loss: 0.0610 - acc: 0.9714 - val_loss: 1.8145 - val_acc: 0.5612\n",
      "Epoch 65/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0668 - acc: 0.9731 - val_loss: 1.8622 - val_acc: 0.5687\n",
      "Epoch 66/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1054 - acc: 0.9554 - val_loss: 1.7687 - val_acc: 0.5763\n",
      "Epoch 67/100\n",
      "1584/1584 [==============================] - 2s 969us/step - loss: 0.0772 - acc: 0.9668 - val_loss: 1.7056 - val_acc: 0.5612\n",
      "Epoch 68/100\n",
      "1584/1584 [==============================] - 2s 974us/step - loss: 0.0565 - acc: 0.9764 - val_loss: 1.7252 - val_acc: 0.5574\n",
      "Epoch 69/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0693 - acc: 0.9699 - val_loss: 1.8440 - val_acc: 0.5556\n",
      "Epoch 70/100\n",
      "1584/1584 [==============================] - 2s 968us/step - loss: 0.0619 - acc: 0.9707 - val_loss: 2.0214 - val_acc: 0.5782\n",
      "Epoch 71/100\n",
      "1584/1584 [==============================] - 2s 958us/step - loss: 0.0667 - acc: 0.9716 - val_loss: 1.9771 - val_acc: 0.5876\n",
      "Epoch 72/100\n",
      "1584/1584 [==============================] - 2s 955us/step - loss: 0.0586 - acc: 0.9731 - val_loss: 1.9059 - val_acc: 0.5782\n",
      "Epoch 73/100\n",
      "1584/1584 [==============================] - 2s 970us/step - loss: 0.0734 - acc: 0.9701 - val_loss: 2.0641 - val_acc: 0.5744\n",
      "Epoch 74/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0633 - acc: 0.9699 - val_loss: 2.1387 - val_acc: 0.5763\n",
      "Epoch 75/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0634 - acc: 0.9714 - val_loss: 1.7960 - val_acc: 0.5631\n",
      "Epoch 76/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0638 - acc: 0.9695 - val_loss: 1.8187 - val_acc: 0.5876\n",
      "Epoch 77/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0623 - acc: 0.9731 - val_loss: 2.0563 - val_acc: 0.5612\n",
      "Epoch 78/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0655 - acc: 0.9716 - val_loss: 2.0544 - val_acc: 0.5800\n",
      "Epoch 79/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0663 - acc: 0.9689 - val_loss: 1.9881 - val_acc: 0.5819\n",
      "Epoch 80/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0526 - acc: 0.9745 - val_loss: 1.9310 - val_acc: 0.5311\n",
      "Epoch 81/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0616 - acc: 0.9716 - val_loss: 2.1752 - val_acc: 0.5819\n",
      "Epoch 82/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0566 - acc: 0.9722 - val_loss: 1.9737 - val_acc: 0.5763\n",
      "Epoch 83/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0537 - acc: 0.9760 - val_loss: 2.0710 - val_acc: 0.5593\n",
      "Epoch 84/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0602 - acc: 0.9731 - val_loss: 2.0642 - val_acc: 0.5725\n",
      "Epoch 85/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0597 - acc: 0.9722 - val_loss: 2.1151 - val_acc: 0.5706\n",
      "Epoch 86/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0593 - acc: 0.9724 - val_loss: 2.0511 - val_acc: 0.5593\n",
      "Epoch 87/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0616 - acc: 0.9710 - val_loss: 2.7008 - val_acc: 0.5782\n",
      "Epoch 88/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0585 - acc: 0.9743 - val_loss: 2.2139 - val_acc: 0.5612\n",
      "Epoch 89/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0853 - acc: 0.9617 - val_loss: 2.0233 - val_acc: 0.5480\n",
      "Epoch 90/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0519 - acc: 0.9764 - val_loss: 2.1277 - val_acc: 0.5857\n",
      "Epoch 91/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0562 - acc: 0.9750 - val_loss: 2.2366 - val_acc: 0.5367\n",
      "Epoch 92/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0558 - acc: 0.9741 - val_loss: 2.2154 - val_acc: 0.5782\n",
      "Epoch 93/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0532 - acc: 0.9747 - val_loss: 2.1036 - val_acc: 0.5556\n",
      "Epoch 94/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0641 - acc: 0.9714 - val_loss: 2.0821 - val_acc: 0.5556\n",
      "Epoch 95/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0600 - acc: 0.9693 - val_loss: 2.5694 - val_acc: 0.5744\n",
      "Epoch 96/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0538 - acc: 0.9754 - val_loss: 2.2257 - val_acc: 0.5443\n",
      "Epoch 97/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0576 - acc: 0.9754 - val_loss: 2.0883 - val_acc: 0.5631\n",
      "Epoch 98/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0511 - acc: 0.9769 - val_loss: 2.3906 - val_acc: 0.5819\n",
      "Epoch 99/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0637 - acc: 0.9680 - val_loss: 2.1767 - val_acc: 0.5687\n",
      "Epoch 100/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0591 - acc: 0.9745 - val_loss: 2.2979 - val_acc: 0.5631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a33941ed0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Dense(units = 15, kernel_initializer='uniform', activation = 'relu'))\n",
    "classifier.add(Dense(units = 15, kernel_initializer='uniform', activation = 'relu'))\n",
    "classifier.add(Dense(units = 3, kernel_initializer='uniform', activation = 'softmax'))\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for no tweet model: 2.09\n",
      "acc for no tweet model: 0.59\n"
     ]
    }
   ],
   "source": [
    "score_no_tweet, acc_no_tweet = classifier.evaluate(X_test, y_test, verbose = 2, batch_size = 32)\n",
    "print(\"score for no tweet model: %.2f\" % (score_no_tweet))\n",
    "print(\"acc for no tweet model: %.2f\" % (acc_no_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network for the non-tweet data has a 59% accuracy rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.clf()\n",
    "\n",
    "# loss_values = model_val_dict['loss']\n",
    "# val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "# epochs = range(1, len(loss_values) + 1)\n",
    "# plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "# plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "# plt.title('Training & validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
