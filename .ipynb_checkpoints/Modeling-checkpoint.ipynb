{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and GLoVe modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 from model_data.csv has tweets that have punctuation and are not lemmatized.\n",
    "# df1 = pd.read_csv('model_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('CLEAN_EDIT_model_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['Unnamed: 0'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_x</th>\n",
       "      <th>retweet_from</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>encoded_sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abort</th>\n",
       "      <th>...</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yr</th>\n",
       "      <th>yup</th>\n",
       "      <th>zero</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stay</td>\n",
       "      <td>137</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>assuming acceleration of to but in a comfortab...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 3858 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  signal_x  retweet_from  tweet_length  encoded_sentiment  polarity  \\\n",
       "0     stay           137           105                  1  0.366667   \n",
       "\n",
       "                                               tweet  ability  able  aboard  \\\n",
       "0  assuming acceleration of to but in a comfortab...      0.0   0.0     0.0   \n",
       "\n",
       "   abort ...   you  your  yours  yourself   yr  yup  zero  zip  zone  zoo  \n",
       "0    0.0 ...   0.0   0.0    0.0       0.0  0.0  0.0   0.0  0.0   0.0  0.0  \n",
       "\n",
       "[1 rows x 3858 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set the tweet column to a string to account of data type errors\n",
    "df.tweet = df.tweet.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the clean dataframe\n",
    "# df.to_csv(r'/Users/sproul/Desktop/ds-projects/project_mod4_AAR/maybe_final_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the class balance of our target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.signal_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stay    882\n",
       "up      679\n",
       "down    641\n",
       "Name: signal_x, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"signal_x\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a histogram to show the distribution of target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stay    0.400545\n",
       "up      0.308356\n",
       "down    0.291099\n",
       "Name: signal_x, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"signal_x\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set target to the daily move in stock price (or signal_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.signal_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    stay\n",
       "1    stay\n",
       "2    stay\n",
       "3    stay\n",
       "4    stay\n",
       "Name: signal_x, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the target to an array of dummies for our three classes (stay, up, down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this y (the target) for all of our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(target).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(y))\n",
    "y[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial ANN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The initial data for our first model is just the text of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['tweet'].map(word_tokenize).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([list(['assuming', 'acceleration', 'of', 'to', 'but', 'in', 'a', 'comfortable', 'direction', 'will', 'feel', 'like', 'a', 'mild', 'to']),\n",
       "       list(['is', 'capable', 'of', 'transporting', 'satellite', 'to', 'orbit', 'crew', 'and', 'cargo', 'to', 'the', 'and', 'mission', 'to', 'the', 'moon', 'an']),\n",
       "       list(['yup']), ...,\n",
       "       list(['these', 'article', 'in', 'space', 'news', 'describe', 'why', 'v', 't', 'and', 't', 'w']),\n",
       "       list(['wa', 'by', 'a', 'saying', 'rocket', 'ha', 'no', 'chance', 'just', 'said', 'the', 'franco', 'german', 'ha', 'no', 'chance', 'so', 'go', 'with']),\n",
       "       list(['just', 'returned', 'from', 'a', 'trip', 'to', 'and', 'where', 'i', 'met', 'with', 'many', 'interesting', 'people', 'i', 'really', 'like'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(data))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the words with keras preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=15000)\n",
    "tokenizer.fit_on_texts(list(df.tweet))\n",
    "list_tokenized_tweets = tokenizer.texts_to_sequences(df.tweet)\n",
    "X_t = sequence.pad_sequences(list_tokenized_tweets) #, maxlen=2709)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a list of arrays of numbers for each tweet\n",
      "<type 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[979, 532, 4, 2, 20, 6, 3, 1784, 1258, 12, 386, 36, 3, 1785, 2],\n",
       " [5, 805, 4, 1259, 184, 2, 135, 416, 8, 457, 2, 1, 8, 151, 2, 1, 285, 46],\n",
       " [387]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('this is a list of arrays of numbers for each tweet')\n",
    "print(type(list_tokenized_tweets))\n",
    "list_tokenized_tweets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(2202, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,  979,  532,    4,    2,   20,    6,    3, 1784, 1258,\n",
       "          12,  386,   36,    3, 1785,    2],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    5,\n",
       "         805,    4, 1259,  184,    2,  135,  416,    8,  457,    2,    1,\n",
       "           8,  151,    2,    1,  285,   46],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,  387]], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(X_t))\n",
    "print(X_t.shape)\n",
    "X_t[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tweet, X_test_tweet, y_train_tweet, y_test_tweet = train_test_split(X_t, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 28 # another test is 128\n",
    "input_ = Input(shape=(28,)) # another test is 100\n",
    "x = Embedding(10000, embedding_size)(input_)\n",
    "x = LSTM(25, return_sequences=True)(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# There are 3 different possible classes, so we use 3 neurons in our output layer\n",
    "x = Dense(3, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 28, 28)            280000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 28, 25)            5400      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                1300      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 286,853\n",
      "Trainable params: 286,853\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1584 samples, validate on 177 samples\n",
      "Epoch 1/20\n",
      "1584/1584 [==============================] - 8s 5ms/step - loss: 1.0961 - acc: 0.3731 - val_loss: 1.0950 - val_acc: 0.3729\n",
      "Epoch 2/20\n",
      "1584/1584 [==============================] - 6s 4ms/step - loss: 1.0901 - acc: 0.4078 - val_loss: 1.0952 - val_acc: 0.3729\n",
      "Epoch 3/20\n",
      "1584/1584 [==============================] - 6s 4ms/step - loss: 1.0871 - acc: 0.4078 - val_loss: 1.0967 - val_acc: 0.3729\n",
      "Epoch 4/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 1.0774 - acc: 0.4091 - val_loss: 1.0942 - val_acc: 0.3729\n",
      "Epoch 5/20\n",
      "1584/1584 [==============================] - 6s 4ms/step - loss: 1.0399 - acc: 0.4602 - val_loss: 1.0963 - val_acc: 0.3672\n",
      "Epoch 6/20\n",
      "1584/1584 [==============================] - 6s 4ms/step - loss: 0.9127 - acc: 0.5612 - val_loss: 1.1358 - val_acc: 0.3842\n",
      "Epoch 7/20\n",
      "1584/1584 [==============================] - 6s 4ms/step - loss: 0.7966 - acc: 0.6199 - val_loss: 1.4368 - val_acc: 0.3333\n",
      "Epoch 8/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 0.6978 - acc: 0.6742 - val_loss: 1.6267 - val_acc: 0.3616\n",
      "Epoch 9/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 0.6149 - acc: 0.7178 - val_loss: 2.0683 - val_acc: 0.3559\n",
      "Epoch 10/20\n",
      "1584/1584 [==============================] - 6s 4ms/step - loss: 0.5034 - acc: 0.7727 - val_loss: 2.1730 - val_acc: 0.3559\n",
      "Epoch 11/20\n",
      "1584/1584 [==============================] - 6s 4ms/step - loss: 0.4399 - acc: 0.8125 - val_loss: 2.2448 - val_acc: 0.3559\n",
      "Epoch 12/20\n",
      "1584/1584 [==============================] - 6s 4ms/step - loss: 0.4146 - acc: 0.8314 - val_loss: 2.9585 - val_acc: 0.3842\n",
      "Epoch 13/20\n",
      "1584/1584 [==============================] - 7s 4ms/step - loss: 0.3662 - acc: 0.8390 - val_loss: 3.2037 - val_acc: 0.3390\n",
      "Epoch 14/20\n",
      "1584/1584 [==============================] - 7s 4ms/step - loss: 0.3261 - acc: 0.8674 - val_loss: 3.2508 - val_acc: 0.3616\n",
      "Epoch 15/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 0.2754 - acc: 0.8984 - val_loss: 3.5745 - val_acc: 0.3446\n",
      "Epoch 16/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 0.2705 - acc: 0.8958 - val_loss: 3.3161 - val_acc: 0.3672\n",
      "Epoch 17/20\n",
      "1584/1584 [==============================] - 6s 4ms/step - loss: 0.2472 - acc: 0.8908 - val_loss: 4.0408 - val_acc: 0.3503\n",
      "Epoch 18/20\n",
      "1584/1584 [==============================] - 5s 3ms/step - loss: 0.2195 - acc: 0.8939 - val_loss: 4.5730 - val_acc: 0.3503\n",
      "Epoch 19/20\n",
      "1584/1584 [==============================] - 6s 4ms/step - loss: 0.2111 - acc: 0.9091 - val_loss: 4.8421 - val_acc: 0.3446\n",
      "Epoch 20/20\n",
      "1584/1584 [==============================] - 6s 4ms/step - loss: 0.2041 - acc: 0.9097 - val_loss: 4.8116 - val_acc: 0.3503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x110fe12d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_tweet, y_train_tweet, epochs=20, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 4.00\n",
      "acc: 0.39\n"
     ]
    }
   ],
   "source": [
    "score_tweet, acc_tweet = model.evaluate(X_test_tweet, y_test_tweet, verbose = 2, batch_size = 32)\n",
    "print(\"score: %.2f\" % (score_tweet))\n",
    "print(\"acc: %.2f\" % (acc_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is the base line Neural Network, with an accuracy of 43%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New model experiments with Random Forest, Support Vector Machine, and Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are testing other models, to get a general baseline before optimizing the Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3235"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocabulary = set(word for tweet in data for word in tweet)\n",
    "len(total_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in that text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open('glove.6B.100d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note from Mike: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # It can't be used in a sklearn Pipeline. \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf =  Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "              (\"Random Forest\", RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "              ('Logistic Regression', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('Random Forest', rf),\n",
    "          (\"Support Vector Machine\", svc),\n",
    "          (\"Logistic Regression\", lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "/Users/roberthillery/anaconda3/envs/venv/lib/python2.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/roberthillery/anaconda3/envs/venv/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/roberthillery/anaconda3/envs/venv/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "scores = [(name, cross_val_score(model, data, target, cv=2).mean()) for name, model, in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.3696658967167134),\n",
       " ('Support Vector Machine', 0.40009074410163337),\n",
       " ('Logistic Regression', 0.36239894406863554)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest cross validation score 0.3696658967167134"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine cross validation score 0.40009074410163337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Logistic Regression cross validation score 0.36239894406863554"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_x</th>\n",
       "      <th>retweet_from</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>encoded_sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abort</th>\n",
       "      <th>...</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yr</th>\n",
       "      <th>yup</th>\n",
       "      <th>zero</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stay</td>\n",
       "      <td>137</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>assuming acceleration of to but in a comfortab...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stay</td>\n",
       "      <td>64</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>is capable of transporting satellite to orbit ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stay</td>\n",
       "      <td>137</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>yup</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stay</td>\n",
       "      <td>137</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>part</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stay</td>\n",
       "      <td>137</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>fly to most place on earth in under min and an...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3858 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  signal_x  retweet_from  tweet_length  encoded_sentiment  polarity  \\\n",
       "0     stay           137           105                  1  0.366667   \n",
       "1     stay            64           113                  1  0.200000   \n",
       "2     stay           137             6                  0  0.000000   \n",
       "3     stay           137             7                  0  0.000000   \n",
       "4     stay           137            96                  1  0.650000   \n",
       "\n",
       "                                               tweet  ability  able  aboard  \\\n",
       "0  assuming acceleration of to but in a comfortab...      0.0   0.0     0.0   \n",
       "1  is capable of transporting satellite to orbit ...      0.0   0.0     0.0   \n",
       "2                                                yup      0.0   0.0     0.0   \n",
       "3                                               part      0.0   0.0     0.0   \n",
       "4  fly to most place on earth in under min and an...      0.0   0.0     0.0   \n",
       "\n",
       "   abort ...   you  your  yours  yourself   yr  yup  zero  zip  zone  zoo  \n",
       "0    0.0 ...   0.0   0.0    0.0       0.0  0.0  0.0   0.0  0.0   0.0  0.0  \n",
       "1    0.0 ...   0.0   0.0    0.0       0.0  0.0  0.0   0.0  0.0   0.0  0.0  \n",
       "2    0.0 ...   0.0   0.0    0.0       0.0  0.0  1.0   0.0  0.0   0.0  0.0  \n",
       "3    0.0 ...   0.0   0.0    0.0       0.0  0.0  0.0   0.0  0.0   0.0  0.0  \n",
       "4    0.0 ...   0.0   0.0    0.0       0.0  0.0  0.0   0.0  0.0   0.0  0.0  \n",
       "\n",
       "[5 rows x 3858 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,  979,  532,    4,    2,   20,    6,    3, 1784, 1258,\n",
       "          12,  386,   36,    3, 1785,    2],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    5,\n",
       "         805,    4, 1259,  184,    2,  135,  416,    8,  457,    2,    1,\n",
       "           8,  151,    2,    1,  285,   46],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,  387]], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the tweet data going into the model\n",
    "X_t[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ann, X_test_ann, y_train_ann, y_test_ann = train_test_split(X_t, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1584 samples, validate on 177 samples\n",
      "Epoch 1/100\n",
      "1584/1584 [==============================] - 1s 933us/step - loss: 0.6386 - acc: 0.6593 - val_loss: 0.6348 - val_acc: 0.6685\n",
      "Epoch 2/100\n",
      "1584/1584 [==============================] - 1s 325us/step - loss: 0.6303 - acc: 0.6654 - val_loss: 0.6371 - val_acc: 0.6591\n",
      "Epoch 3/100\n",
      "1584/1584 [==============================] - 1s 324us/step - loss: 0.6241 - acc: 0.6679 - val_loss: 0.6397 - val_acc: 0.6497\n",
      "Epoch 4/100\n",
      "1584/1584 [==============================] - 1s 326us/step - loss: 0.6215 - acc: 0.6698 - val_loss: 0.6401 - val_acc: 0.6648\n",
      "Epoch 5/100\n",
      "1584/1584 [==============================] - 1s 317us/step - loss: 0.6174 - acc: 0.6738 - val_loss: 0.6629 - val_acc: 0.6441\n",
      "Epoch 6/100\n",
      "1584/1584 [==============================] - 1s 329us/step - loss: 0.6128 - acc: 0.6772 - val_loss: 0.6551 - val_acc: 0.6535\n",
      "Epoch 7/100\n",
      "1584/1584 [==============================] - 1s 320us/step - loss: 0.6059 - acc: 0.6787 - val_loss: 0.6392 - val_acc: 0.6648\n",
      "Epoch 8/100\n",
      "1584/1584 [==============================] - 1s 330us/step - loss: 0.6031 - acc: 0.6846 - val_loss: 0.6458 - val_acc: 0.6723\n",
      "Epoch 9/100\n",
      "1584/1584 [==============================] - 1s 326us/step - loss: 0.5956 - acc: 0.6904 - val_loss: 0.6500 - val_acc: 0.6704\n",
      "Epoch 10/100\n",
      "1584/1584 [==============================] - 1s 334us/step - loss: 0.5944 - acc: 0.6936 - val_loss: 0.6650 - val_acc: 0.6516\n",
      "Epoch 11/100\n",
      "1584/1584 [==============================] - 1s 334us/step - loss: 0.5883 - acc: 0.6953 - val_loss: 0.6613 - val_acc: 0.6554\n",
      "Epoch 12/100\n",
      "1584/1584 [==============================] - 1s 325us/step - loss: 0.5827 - acc: 0.6949 - val_loss: 0.6640 - val_acc: 0.6610\n",
      "Epoch 13/100\n",
      "1584/1584 [==============================] - 1s 333us/step - loss: 0.5833 - acc: 0.6984 - val_loss: 0.6877 - val_acc: 0.6460\n",
      "Epoch 14/100\n",
      "1584/1584 [==============================] - 1s 324us/step - loss: 0.5776 - acc: 0.7022 - val_loss: 0.6967 - val_acc: 0.6441\n",
      "Epoch 15/100\n",
      "1584/1584 [==============================] - 1s 336us/step - loss: 0.5749 - acc: 0.7022 - val_loss: 0.6807 - val_acc: 0.6704\n",
      "Epoch 16/100\n",
      "1584/1584 [==============================] - 1s 322us/step - loss: 0.5691 - acc: 0.7075 - val_loss: 0.6803 - val_acc: 0.6554\n",
      "Epoch 17/100\n",
      "1584/1584 [==============================] - 1s 334us/step - loss: 0.5710 - acc: 0.7037 - val_loss: 0.7216 - val_acc: 0.6271\n",
      "Epoch 18/100\n",
      "1584/1584 [==============================] - 1s 324us/step - loss: 0.5594 - acc: 0.7128 - val_loss: 0.7514 - val_acc: 0.6422\n",
      "Epoch 19/100\n",
      "1584/1584 [==============================] - 1s 328us/step - loss: 0.5604 - acc: 0.7128 - val_loss: 0.7124 - val_acc: 0.6497\n",
      "Epoch 20/100\n",
      "1584/1584 [==============================] - 1s 322us/step - loss: 0.5664 - acc: 0.7138 - val_loss: 0.6958 - val_acc: 0.6554\n",
      "Epoch 21/100\n",
      "1584/1584 [==============================] - 1s 353us/step - loss: 0.5569 - acc: 0.7106 - val_loss: 0.7431 - val_acc: 0.6573\n",
      "Epoch 22/100\n",
      "1584/1584 [==============================] - 1s 327us/step - loss: 0.5507 - acc: 0.7197 - val_loss: 0.7283 - val_acc: 0.6535\n",
      "Epoch 23/100\n",
      "1584/1584 [==============================] - 1s 326us/step - loss: 0.5545 - acc: 0.7161 - val_loss: 0.7205 - val_acc: 0.6478\n",
      "Epoch 24/100\n",
      "1584/1584 [==============================] - 1s 324us/step - loss: 0.5471 - acc: 0.7193 - val_loss: 0.7334 - val_acc: 0.6441\n",
      "Epoch 25/100\n",
      "1584/1584 [==============================] - 1s 365us/step - loss: 0.5434 - acc: 0.7226 - val_loss: 0.7239 - val_acc: 0.6591\n",
      "Epoch 26/100\n",
      "1584/1584 [==============================] - 1s 397us/step - loss: 0.5397 - acc: 0.7224 - val_loss: 0.7469 - val_acc: 0.6554\n",
      "Epoch 27/100\n",
      "1584/1584 [==============================] - 1s 421us/step - loss: 0.5449 - acc: 0.7222 - val_loss: 0.7644 - val_acc: 0.6460\n",
      "Epoch 28/100\n",
      "1584/1584 [==============================] - 1s 475us/step - loss: 0.5463 - acc: 0.7195 - val_loss: 0.7251 - val_acc: 0.6685\n",
      "Epoch 29/100\n",
      "1584/1584 [==============================] - 1s 357us/step - loss: 0.5352 - acc: 0.7271 - val_loss: 0.7684 - val_acc: 0.6478\n",
      "Epoch 30/100\n",
      "1584/1584 [==============================] - 1s 372us/step - loss: 0.5324 - acc: 0.7258 - val_loss: 0.8013 - val_acc: 0.6497\n",
      "Epoch 31/100\n",
      "1584/1584 [==============================] - 1s 412us/step - loss: 0.5289 - acc: 0.7281 - val_loss: 0.8449 - val_acc: 0.6497\n",
      "Epoch 32/100\n",
      "1584/1584 [==============================] - 1s 412us/step - loss: 0.5254 - acc: 0.7296 - val_loss: 0.8241 - val_acc: 0.6422\n",
      "Epoch 33/100\n",
      "1584/1584 [==============================] - 1s 389us/step - loss: 0.5334 - acc: 0.7273 - val_loss: 0.8106 - val_acc: 0.6478\n",
      "Epoch 34/100\n",
      "1584/1584 [==============================] - 1s 374us/step - loss: 0.5269 - acc: 0.7302 - val_loss: 0.8190 - val_acc: 0.6573\n",
      "Epoch 35/100\n",
      "1584/1584 [==============================] - 1s 353us/step - loss: 0.5209 - acc: 0.7313 - val_loss: 0.8144 - val_acc: 0.6478\n",
      "Epoch 36/100\n",
      "1584/1584 [==============================] - 1s 344us/step - loss: 0.5142 - acc: 0.7384 - val_loss: 0.8862 - val_acc: 0.6347\n",
      "Epoch 37/100\n",
      "1584/1584 [==============================] - 0s 272us/step - loss: 0.5220 - acc: 0.7321 - val_loss: 0.8917 - val_acc: 0.6403\n",
      "Epoch 38/100\n",
      "1584/1584 [==============================] - 0s 279us/step - loss: 0.5238 - acc: 0.7285 - val_loss: 0.9634 - val_acc: 0.6252\n",
      "Epoch 39/100\n",
      "1584/1584 [==============================] - 0s 275us/step - loss: 0.5213 - acc: 0.7342 - val_loss: 0.9227 - val_acc: 0.6441\n",
      "Epoch 40/100\n",
      "1584/1584 [==============================] - 0s 277us/step - loss: 0.5241 - acc: 0.7319 - val_loss: 0.9205 - val_acc: 0.6347\n",
      "Epoch 41/100\n",
      "1584/1584 [==============================] - 0s 267us/step - loss: 0.5224 - acc: 0.7323 - val_loss: 0.9042 - val_acc: 0.6591\n",
      "Epoch 42/100\n",
      "1584/1584 [==============================] - 0s 272us/step - loss: 0.5125 - acc: 0.7365 - val_loss: 0.9100 - val_acc: 0.6460\n",
      "Epoch 43/100\n",
      "1584/1584 [==============================] - 0s 275us/step - loss: 0.5109 - acc: 0.7391 - val_loss: 0.9380 - val_acc: 0.6535\n",
      "Epoch 44/100\n",
      "1584/1584 [==============================] - 0s 268us/step - loss: 0.5064 - acc: 0.7410 - val_loss: 0.9634 - val_acc: 0.6309\n",
      "Epoch 45/100\n",
      "1584/1584 [==============================] - 0s 277us/step - loss: 0.5060 - acc: 0.7384 - val_loss: 0.8701 - val_acc: 0.6441\n",
      "Epoch 46/100\n",
      "1584/1584 [==============================] - 0s 277us/step - loss: 0.5163 - acc: 0.7374 - val_loss: 0.9165 - val_acc: 0.6403\n",
      "Epoch 47/100\n",
      "1584/1584 [==============================] - 0s 283us/step - loss: 0.5166 - acc: 0.7336 - val_loss: 0.8526 - val_acc: 0.6629\n",
      "Epoch 48/100\n",
      "1584/1584 [==============================] - 0s 268us/step - loss: 0.5088 - acc: 0.7407 - val_loss: 0.9579 - val_acc: 0.6422\n",
      "Epoch 49/100\n",
      "1584/1584 [==============================] - 0s 275us/step - loss: 0.5117 - acc: 0.7363 - val_loss: 0.9622 - val_acc: 0.6460\n",
      "Epoch 50/100\n",
      "1584/1584 [==============================] - 0s 285us/step - loss: 0.5001 - acc: 0.7449 - val_loss: 0.9872 - val_acc: 0.6403\n",
      "Epoch 51/100\n",
      "1584/1584 [==============================] - 0s 269us/step - loss: 0.5028 - acc: 0.7439 - val_loss: 0.9380 - val_acc: 0.6460\n",
      "Epoch 52/100\n",
      "1584/1584 [==============================] - 0s 276us/step - loss: 0.5168 - acc: 0.7336 - val_loss: 0.9272 - val_acc: 0.6422\n",
      "Epoch 53/100\n",
      "1584/1584 [==============================] - 0s 267us/step - loss: 0.5125 - acc: 0.7370 - val_loss: 0.8517 - val_acc: 0.6629\n",
      "Epoch 54/100\n",
      "1584/1584 [==============================] - 0s 280us/step - loss: 0.5031 - acc: 0.7420 - val_loss: 0.9297 - val_acc: 0.6629\n",
      "Epoch 55/100\n",
      "1584/1584 [==============================] - 0s 264us/step - loss: 0.4986 - acc: 0.7407 - val_loss: 0.9721 - val_acc: 0.6497\n",
      "Epoch 56/100\n",
      "1584/1584 [==============================] - 0s 277us/step - loss: 0.5043 - acc: 0.7447 - val_loss: 0.8502 - val_acc: 0.6554\n",
      "Epoch 57/100\n",
      "1584/1584 [==============================] - 0s 273us/step - loss: 0.5256 - acc: 0.7357 - val_loss: 0.9683 - val_acc: 0.6554\n",
      "Epoch 58/100\n",
      "1584/1584 [==============================] - 0s 268us/step - loss: 0.5028 - acc: 0.7462 - val_loss: 0.9146 - val_acc: 0.6591\n",
      "Epoch 59/100\n",
      "1584/1584 [==============================] - 0s 284us/step - loss: 0.5054 - acc: 0.7397 - val_loss: 0.9358 - val_acc: 0.6591\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1584/1584 [==============================] - 0s 266us/step - loss: 0.4937 - acc: 0.7445 - val_loss: 0.9492 - val_acc: 0.6667\n",
      "Epoch 61/100\n",
      "1584/1584 [==============================] - 0s 283us/step - loss: 0.5103 - acc: 0.7401 - val_loss: 0.9111 - val_acc: 0.6610\n",
      "Epoch 62/100\n",
      "1584/1584 [==============================] - 0s 262us/step - loss: 0.4954 - acc: 0.7449 - val_loss: 0.9176 - val_acc: 0.6629\n",
      "Epoch 63/100\n",
      "1584/1584 [==============================] - 0s 278us/step - loss: 0.4886 - acc: 0.7475 - val_loss: 0.9461 - val_acc: 0.6648\n",
      "Epoch 64/100\n",
      "1584/1584 [==============================] - 0s 274us/step - loss: 0.4889 - acc: 0.7504 - val_loss: 0.9204 - val_acc: 0.6648\n",
      "Epoch 65/100\n",
      "1584/1584 [==============================] - 0s 275us/step - loss: 0.4925 - acc: 0.7498 - val_loss: 0.8993 - val_acc: 0.6798\n",
      "Epoch 66/100\n",
      "1584/1584 [==============================] - 0s 273us/step - loss: 0.4971 - acc: 0.7496 - val_loss: 1.0424 - val_acc: 0.6535\n",
      "Epoch 67/100\n",
      "1584/1584 [==============================] - 0s 274us/step - loss: 0.4908 - acc: 0.7513 - val_loss: 0.8665 - val_acc: 0.6704\n",
      "Epoch 68/100\n",
      "1584/1584 [==============================] - 0s 280us/step - loss: 0.5053 - acc: 0.7466 - val_loss: 1.1059 - val_acc: 0.6610\n",
      "Epoch 69/100\n",
      "1584/1584 [==============================] - 0s 267us/step - loss: 0.5268 - acc: 0.7386 - val_loss: 1.0239 - val_acc: 0.6685\n",
      "Epoch 70/100\n",
      "1584/1584 [==============================] - 0s 272us/step - loss: 0.5078 - acc: 0.7422 - val_loss: 1.1308 - val_acc: 0.6365\n",
      "Epoch 71/100\n",
      "1584/1584 [==============================] - 0s 271us/step - loss: 0.5207 - acc: 0.7382 - val_loss: 1.0695 - val_acc: 0.6535\n",
      "Epoch 72/100\n",
      "1584/1584 [==============================] - 1s 483us/step - loss: 0.4963 - acc: 0.7445 - val_loss: 1.0157 - val_acc: 0.6629\n",
      "Epoch 73/100\n",
      "1584/1584 [==============================] - 1s 382us/step - loss: 0.4886 - acc: 0.7527 - val_loss: 1.0151 - val_acc: 0.6573\n",
      "Epoch 74/100\n",
      "1584/1584 [==============================] - 1s 347us/step - loss: 0.4863 - acc: 0.7517 - val_loss: 1.0212 - val_acc: 0.6591\n",
      "Epoch 75/100\n",
      "1584/1584 [==============================] - 1s 416us/step - loss: 0.4894 - acc: 0.7494 - val_loss: 0.9719 - val_acc: 0.6685\n",
      "Epoch 76/100\n",
      "1584/1584 [==============================] - 1s 338us/step - loss: 0.4839 - acc: 0.7521 - val_loss: 1.0069 - val_acc: 0.6648\n",
      "Epoch 77/100\n",
      "1584/1584 [==============================] - 1s 318us/step - loss: 0.4887 - acc: 0.7519 - val_loss: 0.9859 - val_acc: 0.6780\n",
      "Epoch 78/100\n",
      "1584/1584 [==============================] - 0s 265us/step - loss: 0.4974 - acc: 0.7456 - val_loss: 1.0098 - val_acc: 0.6478\n",
      "Epoch 79/100\n",
      "1584/1584 [==============================] - 0s 279us/step - loss: 0.4951 - acc: 0.7492 - val_loss: 0.9770 - val_acc: 0.6610\n",
      "Epoch 80/100\n",
      "1584/1584 [==============================] - 0s 263us/step - loss: 0.4917 - acc: 0.7532 - val_loss: 1.0070 - val_acc: 0.6798\n",
      "Epoch 81/100\n",
      "1584/1584 [==============================] - 0s 270us/step - loss: 0.4939 - acc: 0.7496 - val_loss: 1.0176 - val_acc: 0.6742\n",
      "Epoch 82/100\n",
      "1584/1584 [==============================] - 0s 283us/step - loss: 0.5037 - acc: 0.7447 - val_loss: 1.0639 - val_acc: 0.6460\n",
      "Epoch 83/100\n",
      "1584/1584 [==============================] - 0s 268us/step - loss: 0.5074 - acc: 0.7494 - val_loss: 0.9765 - val_acc: 0.6685\n",
      "Epoch 84/100\n",
      "1584/1584 [==============================] - 0s 271us/step - loss: 0.4885 - acc: 0.7542 - val_loss: 0.9482 - val_acc: 0.6648\n",
      "Epoch 85/100\n",
      "1584/1584 [==============================] - 0s 265us/step - loss: 0.4880 - acc: 0.7548 - val_loss: 0.9726 - val_acc: 0.6573\n",
      "Epoch 86/100\n",
      "1584/1584 [==============================] - 0s 283us/step - loss: 0.4818 - acc: 0.7555 - val_loss: 1.0370 - val_acc: 0.6648\n",
      "Epoch 87/100\n",
      "1584/1584 [==============================] - 0s 269us/step - loss: 0.4874 - acc: 0.7515 - val_loss: 1.1082 - val_acc: 0.6629\n",
      "Epoch 88/100\n",
      "1584/1584 [==============================] - 0s 273us/step - loss: 0.4940 - acc: 0.7496 - val_loss: 0.9936 - val_acc: 0.6591\n",
      "Epoch 89/100\n",
      "1584/1584 [==============================] - 0s 276us/step - loss: 0.4907 - acc: 0.7508 - val_loss: 1.0390 - val_acc: 0.6723\n",
      "Epoch 90/100\n",
      "1584/1584 [==============================] - 0s 269us/step - loss: 0.4979 - acc: 0.7513 - val_loss: 0.9892 - val_acc: 0.6591\n",
      "Epoch 91/100\n",
      "1584/1584 [==============================] - 0s 276us/step - loss: 0.4809 - acc: 0.7551 - val_loss: 1.0675 - val_acc: 0.6610\n",
      "Epoch 92/100\n",
      "1584/1584 [==============================] - 0s 266us/step - loss: 0.5006 - acc: 0.7460 - val_loss: 1.0421 - val_acc: 0.6685\n",
      "Epoch 93/100\n",
      "1584/1584 [==============================] - 0s 283us/step - loss: 0.4943 - acc: 0.7473 - val_loss: 1.0500 - val_acc: 0.6685\n",
      "Epoch 94/100\n",
      "1584/1584 [==============================] - 0s 267us/step - loss: 0.4973 - acc: 0.7475 - val_loss: 0.8925 - val_acc: 0.6591\n",
      "Epoch 95/100\n",
      "1584/1584 [==============================] - 0s 271us/step - loss: 0.5152 - acc: 0.7380 - val_loss: 1.0624 - val_acc: 0.6573\n",
      "Epoch 96/100\n",
      "1584/1584 [==============================] - 0s 271us/step - loss: 0.5022 - acc: 0.7447 - val_loss: 1.0351 - val_acc: 0.6554\n",
      "Epoch 97/100\n",
      "1584/1584 [==============================] - 0s 274us/step - loss: 0.4889 - acc: 0.7525 - val_loss: 1.1198 - val_acc: 0.6422\n",
      "Epoch 98/100\n",
      "1584/1584 [==============================] - 0s 277us/step - loss: 0.4795 - acc: 0.7540 - val_loss: 1.1666 - val_acc: 0.6573\n",
      "Epoch 99/100\n",
      "1584/1584 [==============================] - 0s 278us/step - loss: 0.4851 - acc: 0.7565 - val_loss: 0.9091 - val_acc: 0.6780\n",
      "Epoch 100/100\n",
      "1584/1584 [==============================] - 0s 291us/step - loss: 0.5077 - acc: 0.7456 - val_loss: 1.0736 - val_acc: 0.6610\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Dense(units = 15, kernel_initializer='uniform', activation = 'relu'))\n",
    "classifier.add(Dense(units = 15, kernel_initializer='uniform', activation = 'relu'))\n",
    "classifier.add(Dense(units = 3, kernel_initializer='uniform', activation = 'softmax'))\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# ann = classifier.fit(train_final, label_train_final, batch_size = 10, epochs = 100, validation_data=(val, label_val))\n",
    "ann = classifier.fit(X_train_ann, y_train_ann, batch_size = 10, epochs = 100, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1761/1761 [==============================] - 0s 32us/step\n"
     ]
    }
   ],
   "source": [
    "results_train_ann = classifier.evaluate(X_train_ann, y_train_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441/441 [==============================] - 0s 38us/step\n"
     ]
    }
   ],
   "source": [
    "results_test_ann = classifier.evaluate(X_test_ann, y_test_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5391556257757526, 0.7482490954854013]\n",
      "[1.2146780242184663, 0.6243386186845178]\n"
     ]
    }
   ],
   "source": [
    "print(results_train_ann)\n",
    "print(results_test_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for ann model: 1.21\n",
      "acc for ann model: 0.62\n"
     ]
    }
   ],
   "source": [
    "score_ann, acc_ann = classifier.evaluate(X_test_ann, y_test_ann, verbose = 2, batch_size = 32)\n",
    "print(\"score for ann model: %.2f\" % (score_ann))\n",
    "print(\"acc for ann model: %.2f\" % (acc_ann))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The neural network for tweets has a 62% accuracy rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN for non tweet features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the dataframe for not tweet features, which include the tf-idf of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_tweet = df.drop(['signal_x', 'tweet'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweet_from</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>encoded_sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abort</th>\n",
       "      <th>aborted</th>\n",
       "      <th>about</th>\n",
       "      <th>...</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yr</th>\n",
       "      <th>yup</th>\n",
       "      <th>zero</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>137</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3856 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   retweet_from  tweet_length  encoded_sentiment  polarity  ability  able  \\\n",
       "0           137           105                  1  0.366667      0.0   0.0   \n",
       "1            64           113                  1  0.200000      0.0   0.0   \n",
       "2           137             6                  0  0.000000      0.0   0.0   \n",
       "3           137             7                  0  0.000000      0.0   0.0   \n",
       "4           137            96                  1  0.650000      0.0   0.0   \n",
       "\n",
       "   aboard  abort  aborted  about ...   you  your  yours  yourself   yr  yup  \\\n",
       "0     0.0    0.0      0.0    0.0 ...   0.0   0.0    0.0       0.0  0.0  0.0   \n",
       "1     0.0    0.0      0.0    0.0 ...   0.0   0.0    0.0       0.0  0.0  0.0   \n",
       "2     0.0    0.0      0.0    0.0 ...   0.0   0.0    0.0       0.0  0.0  1.0   \n",
       "3     0.0    0.0      0.0    0.0 ...   0.0   0.0    0.0       0.0  0.0  0.0   \n",
       "4     0.0    0.0      0.0    0.0 ...   0.0   0.0    0.0       0.0  0.0  0.0   \n",
       "\n",
       "   zero  zip  zone  zoo  \n",
       "0   0.0  0.0   0.0  0.0  \n",
       "1   0.0  0.0   0.0  0.0  \n",
       "2   0.0  0.0   0.0  0.0  \n",
       "3   0.0  0.0   0.0  0.0  \n",
       "4   0.0  0.0   0.0  0.0  \n",
       "\n",
       "[5 rows x 3856 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roberthillery/anaconda3/envs/venv/lib/python2.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# these are the features for the no_tweet ANN model\n",
    "X = df_no_tweet.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[137., 105.,   1., ...,   0.,   0.,   0.],\n",
       "       [ 64., 113.,   1., ...,   0.,   0.,   0.],\n",
       "       [137.,   6.,   0., ...,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# this is check the target for the Neural Network\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1584 samples, validate on 177 samples\n",
      "Epoch 1/100\n",
      "1584/1584 [==============================] - 3s 2ms/step - loss: 0.6326 - acc: 0.6667 - val_loss: 0.6386 - val_acc: 0.6667\n",
      "Epoch 2/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6304 - acc: 0.6667 - val_loss: 0.6381 - val_acc: 0.6667\n",
      "Epoch 3/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6312 - acc: 0.6667 - val_loss: 0.6373 - val_acc: 0.6667\n",
      "Epoch 4/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6299 - acc: 0.6667 - val_loss: 0.6384 - val_acc: 0.6667\n",
      "Epoch 5/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6297 - acc: 0.6667 - val_loss: 0.6385 - val_acc: 0.6667\n",
      "Epoch 6/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6297 - acc: 0.6660 - val_loss: 0.6370 - val_acc: 0.6667\n",
      "Epoch 7/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6289 - acc: 0.6667 - val_loss: 0.6398 - val_acc: 0.6667\n",
      "Epoch 8/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6282 - acc: 0.6667 - val_loss: 0.6392 - val_acc: 0.6667\n",
      "Epoch 9/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6272 - acc: 0.6671 - val_loss: 0.6382 - val_acc: 0.6667\n",
      "Epoch 10/100\n",
      "1584/1584 [==============================] - ETA: 0s - loss: 0.6246 - acc: 0.667 - 2s 1ms/step - loss: 0.6247 - acc: 0.6677 - val_loss: 0.6417 - val_acc: 0.6667\n",
      "Epoch 11/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6229 - acc: 0.6690 - val_loss: 0.6389 - val_acc: 0.6667\n",
      "Epoch 12/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.6172 - acc: 0.6732 - val_loss: 0.6412 - val_acc: 0.6667\n",
      "Epoch 13/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.5987 - acc: 0.6974 - val_loss: 0.6427 - val_acc: 0.6573\n",
      "Epoch 14/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.5502 - acc: 0.7254 - val_loss: 0.6735 - val_acc: 0.6516\n",
      "Epoch 15/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.4911 - acc: 0.7506 - val_loss: 0.8681 - val_acc: 0.5838\n",
      "Epoch 16/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.4638 - acc: 0.7584 - val_loss: 0.7132 - val_acc: 0.6460\n",
      "Epoch 17/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.4337 - acc: 0.7713 - val_loss: 0.7435 - val_acc: 0.6252\n",
      "Epoch 18/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.4178 - acc: 0.7811 - val_loss: 0.7565 - val_acc: 0.6347\n",
      "Epoch 19/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.3998 - acc: 0.7961 - val_loss: 0.7726 - val_acc: 0.6328\n",
      "Epoch 20/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.3775 - acc: 0.8190 - val_loss: 0.8423 - val_acc: 0.5989\n",
      "Epoch 21/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.3658 - acc: 0.8260 - val_loss: 0.8034 - val_acc: 0.6083\n",
      "Epoch 22/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.3351 - acc: 0.8580 - val_loss: 0.8974 - val_acc: 0.6026\n",
      "Epoch 23/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.3273 - acc: 0.8559 - val_loss: 0.9595 - val_acc: 0.5895\n",
      "Epoch 24/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.3075 - acc: 0.8695 - val_loss: 0.8612 - val_acc: 0.5857\n",
      "Epoch 25/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.2607 - acc: 0.9005 - val_loss: 0.9055 - val_acc: 0.5876\n",
      "Epoch 26/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.2558 - acc: 0.9005 - val_loss: 0.9404 - val_acc: 0.5913\n",
      "Epoch 27/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.2229 - acc: 0.9165 - val_loss: 0.9550 - val_acc: 0.5800\n",
      "Epoch 28/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.2216 - acc: 0.9095 - val_loss: 1.0945 - val_acc: 0.5744\n",
      "Epoch 29/100\n",
      "1584/1584 [==============================] - 3s 2ms/step - loss: 0.1995 - acc: 0.9158 - val_loss: 0.9811 - val_acc: 0.5687\n",
      "Epoch 30/100\n",
      "1584/1584 [==============================] - 3s 2ms/step - loss: 0.1838 - acc: 0.9274 - val_loss: 1.0091 - val_acc: 0.5800\n",
      "Epoch 31/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1524 - acc: 0.9472 - val_loss: 1.0821 - val_acc: 0.5631\n",
      "Epoch 32/100\n",
      "1584/1584 [==============================] - 2s 2ms/step - loss: 0.1519 - acc: 0.9402 - val_loss: 1.1832 - val_acc: 0.5763\n",
      "Epoch 33/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1396 - acc: 0.9470 - val_loss: 1.1431 - val_acc: 0.5687\n",
      "Epoch 34/100\n",
      "1584/1584 [==============================] - 3s 2ms/step - loss: 0.1767 - acc: 0.9249 - val_loss: 1.1531 - val_acc: 0.5838\n",
      "Epoch 35/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1240 - acc: 0.9543 - val_loss: 1.4091 - val_acc: 0.5518\n",
      "Epoch 36/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1291 - acc: 0.9527 - val_loss: 1.2418 - val_acc: 0.5669\n",
      "Epoch 37/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1114 - acc: 0.9554 - val_loss: 1.2390 - val_acc: 0.5725\n",
      "Epoch 38/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1174 - acc: 0.9518 - val_loss: 1.3010 - val_acc: 0.5800\n",
      "Epoch 39/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1066 - acc: 0.9566 - val_loss: 1.3268 - val_acc: 0.5612\n",
      "Epoch 40/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0992 - acc: 0.9585 - val_loss: 1.3234 - val_acc: 0.5819\n",
      "Epoch 41/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0936 - acc: 0.9632 - val_loss: 1.3653 - val_acc: 0.5669\n",
      "Epoch 42/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0879 - acc: 0.9632 - val_loss: 1.4094 - val_acc: 0.5706\n",
      "Epoch 43/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0821 - acc: 0.9672 - val_loss: 1.4819 - val_acc: 0.5706\n",
      "Epoch 44/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0896 - acc: 0.9651 - val_loss: 1.4885 - val_acc: 0.5687\n",
      "Epoch 45/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1010 - acc: 0.9564 - val_loss: 1.4881 - val_acc: 0.5687\n",
      "Epoch 46/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0857 - acc: 0.9611 - val_loss: 1.4632 - val_acc: 0.5612\n",
      "Epoch 47/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0764 - acc: 0.9659 - val_loss: 1.5743 - val_acc: 0.5782\n",
      "Epoch 48/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0794 - acc: 0.9682 - val_loss: 1.4635 - val_acc: 0.5687\n",
      "Epoch 49/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.1060 - acc: 0.9539 - val_loss: 1.5978 - val_acc: 0.5838\n",
      "Epoch 50/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0813 - acc: 0.9632 - val_loss: 1.7768 - val_acc: 0.5744\n",
      "Epoch 51/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0755 - acc: 0.9691 - val_loss: 1.7608 - val_acc: 0.5725\n",
      "Epoch 52/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0746 - acc: 0.9678 - val_loss: 1.6417 - val_acc: 0.5819\n",
      "Epoch 53/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0728 - acc: 0.9661 - val_loss: 1.6847 - val_acc: 0.5744\n",
      "Epoch 54/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0770 - acc: 0.9672 - val_loss: 1.6108 - val_acc: 0.5480\n",
      "Epoch 55/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0801 - acc: 0.9651 - val_loss: 1.8651 - val_acc: 0.5782\n",
      "Epoch 56/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0715 - acc: 0.9701 - val_loss: 1.7666 - val_acc: 0.5763\n",
      "Epoch 57/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0719 - acc: 0.9684 - val_loss: 1.6470 - val_acc: 0.5593\n",
      "Epoch 58/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0724 - acc: 0.9674 - val_loss: 1.8226 - val_acc: 0.5669\n",
      "Epoch 59/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0755 - acc: 0.9661 - val_loss: 2.1350 - val_acc: 0.5782\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0803 - acc: 0.9657 - val_loss: 1.7735 - val_acc: 0.5518\n",
      "Epoch 61/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0650 - acc: 0.9701 - val_loss: 2.0262 - val_acc: 0.5800\n",
      "Epoch 62/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0726 - acc: 0.9697 - val_loss: 1.7778 - val_acc: 0.5593\n",
      "Epoch 63/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0710 - acc: 0.9672 - val_loss: 1.8701 - val_acc: 0.5593\n",
      "Epoch 64/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0621 - acc: 0.9743 - val_loss: 1.7668 - val_acc: 0.5499\n",
      "Epoch 65/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0575 - acc: 0.9714 - val_loss: 1.8917 - val_acc: 0.5537\n",
      "Epoch 66/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0762 - acc: 0.9680 - val_loss: 2.0094 - val_acc: 0.5650\n",
      "Epoch 67/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0841 - acc: 0.9640 - val_loss: 1.7231 - val_acc: 0.5574\n",
      "Epoch 68/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0607 - acc: 0.9720 - val_loss: 1.9096 - val_acc: 0.5706\n",
      "Epoch 69/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0645 - acc: 0.9676 - val_loss: 1.9378 - val_acc: 0.5650\n",
      "Epoch 70/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0574 - acc: 0.9762 - val_loss: 1.8480 - val_acc: 0.5499\n",
      "Epoch 71/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0846 - acc: 0.9644 - val_loss: 1.9628 - val_acc: 0.5800\n",
      "Epoch 72/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0632 - acc: 0.9703 - val_loss: 1.9207 - val_acc: 0.5631\n",
      "Epoch 73/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0753 - acc: 0.9676 - val_loss: 2.2227 - val_acc: 0.5819\n",
      "Epoch 74/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0625 - acc: 0.9689 - val_loss: 2.0780 - val_acc: 0.5650\n",
      "Epoch 75/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0588 - acc: 0.9729 - val_loss: 1.9498 - val_acc: 0.5687\n",
      "Epoch 76/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0605 - acc: 0.9699 - val_loss: 1.9280 - val_acc: 0.5819\n",
      "Epoch 77/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0623 - acc: 0.9712 - val_loss: 2.2371 - val_acc: 0.5574\n",
      "Epoch 78/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0640 - acc: 0.9680 - val_loss: 2.0079 - val_acc: 0.5706\n",
      "Epoch 79/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0696 - acc: 0.9680 - val_loss: 1.9801 - val_acc: 0.5669\n",
      "Epoch 80/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0974 - acc: 0.9583 - val_loss: 1.8824 - val_acc: 0.5631\n",
      "Epoch 81/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0649 - acc: 0.9722 - val_loss: 2.1296 - val_acc: 0.5687\n",
      "Epoch 82/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0524 - acc: 0.9743 - val_loss: 1.9813 - val_acc: 0.5537\n",
      "Epoch 83/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0536 - acc: 0.9745 - val_loss: 2.1883 - val_acc: 0.5631\n",
      "Epoch 84/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0634 - acc: 0.9701 - val_loss: 2.1368 - val_acc: 0.5480\n",
      "Epoch 85/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0648 - acc: 0.9689 - val_loss: 2.1586 - val_acc: 0.5499\n",
      "Epoch 86/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0544 - acc: 0.9743 - val_loss: 2.1375 - val_acc: 0.5593\n",
      "Epoch 87/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0652 - acc: 0.9693 - val_loss: 2.3945 - val_acc: 0.5612\n",
      "Epoch 88/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0894 - acc: 0.9621 - val_loss: 1.9843 - val_acc: 0.5687\n",
      "Epoch 89/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0558 - acc: 0.9762 - val_loss: 2.0539 - val_acc: 0.5687\n",
      "Epoch 90/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0548 - acc: 0.9758 - val_loss: 2.1203 - val_acc: 0.5782\n",
      "Epoch 91/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0599 - acc: 0.9745 - val_loss: 2.0900 - val_acc: 0.5612\n",
      "Epoch 92/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0656 - acc: 0.9672 - val_loss: 2.0265 - val_acc: 0.5650\n",
      "Epoch 93/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0554 - acc: 0.9735 - val_loss: 2.2942 - val_acc: 0.5537\n",
      "Epoch 94/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0669 - acc: 0.9693 - val_loss: 1.9884 - val_acc: 0.5593\n",
      "Epoch 95/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0573 - acc: 0.9737 - val_loss: 2.0905 - val_acc: 0.5913\n",
      "Epoch 96/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0524 - acc: 0.9760 - val_loss: 2.1117 - val_acc: 0.5650\n",
      "Epoch 97/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0593 - acc: 0.9756 - val_loss: 2.0888 - val_acc: 0.5593\n",
      "Epoch 98/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0529 - acc: 0.9731 - val_loss: 2.1902 - val_acc: 0.5857\n",
      "Epoch 99/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0719 - acc: 0.9684 - val_loss: 2.3282 - val_acc: 0.5669\n",
      "Epoch 100/100\n",
      "1584/1584 [==============================] - 2s 1ms/step - loss: 0.0627 - acc: 0.9741 - val_loss: 2.2454 - val_acc: 0.5838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3c093990>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Dense(units = 15, kernel_initializer='uniform', activation = 'relu'))\n",
    "classifier.add(Dense(units = 15, kernel_initializer='uniform', activation = 'relu'))\n",
    "classifier.add(Dense(units = 3, kernel_initializer='uniform', activation = 'softmax'))\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_no_tweet, acc_no_tweet = classifier.evaluate(X_test, y_test, verbose = 2, batch_size = 32)\n",
    "print(\"score for no tweet model: %.2f\" % (score_no_tweet))\n",
    "print(\"acc for no tweet model: %.2f\" % (acc_no_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network for the non-tweet data has a 59% accuracy rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.clf()\n",
    "\n",
    "# loss_values = model_val_dict['loss']\n",
    "# val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "# epochs = range(1, len(loss_values) + 1)\n",
    "# plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "# plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "# plt.title('Training & validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
