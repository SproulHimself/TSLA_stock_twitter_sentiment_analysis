{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('model_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['Unnamed: 0'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Assuming max acceleration of 2 to 3 g's, but i...\n",
       "1       BFR is capable of transporting satellites to o...\n",
       "2                                                  Yup :)\n",
       "3                                                 Part 2 \n",
       "4       Fly to most places on Earth in under 30 mins a...\n",
       "5       Supporting the creation of a permanent, self-s...\n",
       "6       BFR will take you anywhere on Earth in less th...\n",
       "7       Mars City Opposite of Earth. Dawn and dusk sky...\n",
       "8                                        Moon Base Alpha \n",
       "9       Will be announcing something really special at...\n",
       "10      Nine years ago today, Falcon 1 became the firs...\n",
       "11                         Just another day in the office\n",
       "12                              Congrats Mom! I love you.\n",
       "13      .I'm so excited to say that I'm now officially...\n",
       "14      Prev ideas for paying ~$10B dev cost incl. Kic...\n",
       "15      Headed to Adelaide soon to describe new BFR pl...\n",
       "16                                          Yes, I did :)\n",
       "17      Good NYT article from several years ago about ...\n",
       "18                                                    Yes\n",
       "19      Simulation of how the SpaceX Interplanetary Sp...\n",
       "20                                                   Good\n",
       "21      Major improvements & some unexpected applicati...\n",
       "22      Presentation of Interplanetary Spaceship & Roc...\n",
       "23      Shocking winner of our latest, World's Greates...\n",
       "24      Don't give up if the cause is important enough...\n",
       "25      Absolutely. Moreover, we expect to expand the ...\n",
       "26                                           Of course :)\n",
       "27                                   High res version at \n",
       "28                                             Most of it\n",
       "29      Long road to reusabity of Falcon 9 primary boo...\n",
       "                              ...                        \n",
       "2172    Cool video recap of the big events of the year...\n",
       "2173    Relative to kerosene, methane is not prone to ...\n",
       "2174    Yup or Douglas Aircraft, creator of the awesom...\n",
       "2175    My big worry would be long (hopefully) term wh...\n",
       "2176    SpaceX will go public at some point, as I thin...\n",
       "2177    Best to study some form of engineering, which ...\n",
       "2178    Tesla articles 30 mins apart: This Stock is Sc...\n",
       "2179    YES! ?: BREAKING: to launch anti-cholera initi...\n",
       "2180    Tesla was just approved for a full Class 1 Mas...\n",
       "2181    Told my kids (age 8 and 6) that we were going ...\n",
       "2182    Interesting possible answer to the Fermi Parad...\n",
       "2183    New government study says we could save billio...\n",
       "2184    Am happy to report that Tesla was narrowly cas...\n",
       "2185                                      Thanks Shervin!\n",
       "2186    Mexico is changing in ways that will profoundl...\n",
       "2187    Can't put my finger on it, but for some reason...\n",
       "2188    But if humanity wishes to become a multi-plane...\n",
       "2189    And, yes, I do in fact know that this sounds c...\n",
       "2190    Millions of people needed for Mars colony, so ...\n",
       "2191    My talk for the Royal Aeronautical Society is ...\n",
       "2192    Liam Neeson's Life's Too Short sketch is super...\n",
       "2193                                             Exactly!\n",
       "2194    Love this picture of the Curiousity rover on M...\n",
       "2195    HELLO?.  ?: BREAKING: Is it possible that NASA...\n",
       "2196    Btw, I don't think Apple is doomed. Just won't...\n",
       "2197    Mass. judge denies auto dealers' demand to kil...\n",
       "2198    Amos's article was fair, but his editor chose ...\n",
       "2199    These articles in Space News describe why Aria...\n",
       "2200    Was misquoted by BBC as saying Europe's rocket...\n",
       "2201    Just returned from a trip to London and Oxford...\n",
       "Name: tweet, Length: 2202, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.signal_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['tweet'].map(word_tokenize).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['Assuming', 'max', 'acceleration', 'of', '2', 'to', '3', 'g', \"'s\", ',', 'but', 'in', 'a', 'comfortable', 'direction', '.', 'Will', 'feel', 'like', 'a', 'mild', 'to', 'moder', '?']),\n",
       "       list(['BFR', 'is', 'capable', 'of', 'transporting', 'satellites', 'to', 'orbit', ',', 'crew', 'and', 'cargo', 'to', 'the', 'and', 'completing', 'missions', 'to', 'the', 'Moon', 'an', '?']),\n",
       "       list(['Yup', ':', ')']), ...,\n",
       "       list(['These', 'articles', 'in', 'Space', 'News', 'describe', 'why', 'Ariane', '6', 'vs', '5', ':', 'http', ':', '//t.co/IaeYXXK9', 'and', 'http', ':', '//t.co/ghs3FG8w']),\n",
       "       list(['Was', 'misquoted', 'by', 'BBC', 'as', 'saying', 'Europe', \"'s\", 'rocket', 'has', 'no', 'chance', '.', 'Just', 'said', 'the', '[', 'Franco-German', ']', 'Ariane', '5', 'has', 'no', 'chance', ',', 'so', 'go', 'with', 'Ariane', '6', '.']),\n",
       "       list(['Just', 'returned', 'from', 'a', 'trip', 'to', 'London', 'and', 'Oxford', ',', 'where', 'I', 'met', 'with', 'many', 'interesting', 'people', '.', 'I', 'really', 'like', 'Britain', '!'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(target).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=15000)\n",
    "tokenizer.fit_on_texts(list(df.tweet))\n",
    "list_tokenized_tweets = tokenizer.texts_to_sequences(df.tweet)\n",
    "X_t = sequence.pad_sequences(list_tokenized_tweets, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "input_ = Input(shape=(100,))\n",
    "x = Embedding(10000, embedding_size)(input_)\n",
    "x = LSTM(25, return_sequences=True)(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# There are 41 different possible classes, so we use 41 neurons in our output layer\n",
    "x = Dense(3, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_size = 128\n",
    "# input_ = Input(shape=(100,))\n",
    "# x = Embedding(10000, embedding_size)(input_)\n",
    "# x = LSTM(25, return_sequences=True)(x)\n",
    "# x = GlobalMaxPool1D()(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(50, activation='relu')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# # There are 41 different possible classes, so we use 41 neurons in our output layer\n",
    "# x = Dense(3, activation='relu')(x)\n",
    "\n",
    "# model = Model(inputs=input_, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 100, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100, 25)           15400     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50)                1300      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 1,296,853\n",
      "Trainable params: 1,296,853\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1981 samples, validate on 221 samples\n",
      "Epoch 1/20\n",
      "1981/1981 [==============================] - 7s 3ms/step - loss: nan - acc: 0.2857 - val_loss: nan - val_acc: 0.3575\n",
      "Epoch 2/20\n",
      "1981/1981 [==============================] - 5s 3ms/step - loss: nan - acc: 0.2837 - val_loss: nan - val_acc: 0.3575\n",
      "Epoch 3/20\n",
      "1981/1981 [==============================] - 5s 3ms/step - loss: nan - acc: 0.2837 - val_loss: nan - val_acc: 0.3575\n",
      "Epoch 4/20\n",
      "1981/1981 [==============================] - 5s 3ms/step - loss: nan - acc: 0.2837 - val_loss: nan - val_acc: 0.3575\n",
      "Epoch 5/20\n",
      "1981/1981 [==============================] - 5s 3ms/step - loss: nan - acc: 0.2837 - val_loss: nan - val_acc: 0.3575\n",
      "Epoch 6/20\n",
      "1981/1981 [==============================] - 5s 3ms/step - loss: nan - acc: 0.2837 - val_loss: nan - val_acc: 0.3575\n",
      "Epoch 7/20\n",
      "1981/1981 [==============================] - 6s 3ms/step - loss: nan - acc: 0.2837 - val_loss: nan - val_acc: 0.3575\n",
      "Epoch 8/20\n",
      "1981/1981 [==============================] - 6s 3ms/step - loss: nan - acc: 0.2837 - val_loss: nan - val_acc: 0.3575\n",
      "Epoch 9/20\n",
      "1981/1981 [==============================] - 5s 3ms/step - loss: nan - acc: 0.2837 - val_loss: nan - val_acc: 0.3575\n",
      "Epoch 10/20\n",
      "1981/1981 [==============================] - 5s 3ms/step - loss: nan - acc: 0.2837 - val_loss: nan - val_acc: 0.3575\n",
      "Epoch 11/20\n",
      "1981/1981 [==============================] - 5s 3ms/step - loss: nan - acc: 0.2837 - val_loss: nan - val_acc: 0.3575\n",
      "Epoch 12/20\n",
      "1981/1981 [==============================] - 5s 3ms/step - loss: nan - acc: 0.2837 - val_loss: nan - val_acc: 0.3575\n",
      "Epoch 13/20\n",
      "1981/1981 [==============================] - 6s 3ms/step - loss: nan - acc: 0.2837 - val_loss: nan - val_acc: 0.3575\n",
      "Epoch 14/20\n",
      "1981/1981 [==============================] - 6s 3ms/step - loss: nan - acc: 0.2837 - val_loss: nan - val_acc: 0.3575\n",
      "Epoch 15/20\n",
      "1981/1981 [==============================] - 6s 3ms/step - loss: nan - acc: 0.2837 - val_loss: nan - val_acc: 0.3575\n",
      "Epoch 16/20\n",
      "1981/1981 [==============================] - 5s 3ms/step - loss: nan - acc: 0.2837 - val_loss: nan - val_acc: 0.3575\n",
      "Epoch 17/20\n",
      "1981/1981 [==============================] - 5s 3ms/step - loss: nan - acc: 0.2837 - val_loss: nan - val_acc: 0.3575\n",
      "Epoch 18/20\n",
      "1981/1981 [==============================] - 5s 3ms/step - loss: nan - acc: 0.2837 - val_loss: nan - val_acc: 0.3575\n",
      "Epoch 19/20\n",
      "1981/1981 [==============================] - 5s 3ms/step - loss: nan - acc: 0.2837 - val_loss: nan - val_acc: 0.3575\n",
      "Epoch 20/20\n",
      "1981/1981 [==============================] - 5s 3ms/step - loss: nan - acc: 0.2837 - val_loss: nan - val_acc: 0.3575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a38e754e0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y, epochs=20, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New model experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6501"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set(word for tweet in data for word in tweet)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open('glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
